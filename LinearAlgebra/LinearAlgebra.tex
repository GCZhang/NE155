%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Linear Algebra Lecture
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Class 4 and 5, S14 \\
Types of Equations cont'd + TE + DE \\ January 29 and 31, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

%-------------------------------------------------------------
%-------------------------------------------------------------
\section{Vector Review}

A real $n$-dimensional vector $\vec{x}$ is an ordered set of $n$ real numbers that expresses magnitude and direction:
%
\begin{equation}
\vec{x} = (x_1, x_2, \dots, x_n) \nonumber
\end{equation}

\textbf{Properties}:
%
\begin{enumerate}
\item sum: two vectors of the same size give a new vector of that size: $\vec{x} + \vec{y} = (x_1 + y_1, x_2 + y_2, \dots, x_n + y_n)$

\item scalar multiple: $c\vec{x} = (cx_1, cx_2, \dots, cx_n)$

\item Euclidean norm (length): $||\vec{x}|| = (x_1^2 + x_2^2 + \dots + x_n^2)^{1/2}$

\item dot product: takes two equal length vectors and results in a scalar. 

Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers: $\vec{x} \cdot \vec{y} = \sum_{i=1}^n a_i b_i = x_1 y_1 + x_2 y_2 + \dots + x_n y_n$

Geometrically, it is the product of the magnitudes of the two vectors and the cosine of the angle between them. $\vec{x} \cdot \vec{y} = ||\vec{x}|| ||\vec{y}|| cos\theta$.

\item $||\vec{x}||^2 = \vec{x} \cdot \vec{x}$

\item distance from $\vec{x}$ to $\vec{y}$: $||\vec{x} - \vec{y}|| = ((x_1 - y_1)^2 + (x_2 - y_2)^2 + \dots + (x_n - y_n)^2)^{1/2}$

\item commutative property: $\vec{x} + \vec{y} = \vec{y} + \vec{x}$

\item associative property: $(\vec{x} + \vec{y}) + \vec{z} = \vec{x} + (\vec{y} + \vec{z})$

\item distributive property: $c(\vec{x} + \vec{y}) = c\vec{x} + c\vec{y}$
\end{enumerate}

%-------------------------------------------------------------
%-------------------------------------------------------------
\section{Matrices}

\begin{align}
    \ve{A} &= [a_{ij}]_{m\times n}   =    \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1j} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2j} & \cdots & a_{2n} \\
       \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\     
      a_{31} & a_{32} & \cdots & a_{ij} & \cdots & a_{in} \\
      \vdots & \vdots & \ddots & \vdots & \ddots   & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mj} & \cdots & a_{mn} \\
    \end{pmatrix} \nonumber   
\end{align} 
%
where $i = 1, \dots, m$ is the row index and $j = 1, \dots, n$ is the column index.

$\ve{A} \in \mathbb{R}^{m \times n}$ is an $m \times n$ real matrix\\
$\ve{A} \in \mathbb{C}^{m \times n}$ is an $m \times n$ complex matrix

\textbf{Properties}:
%
\begin{enumerate}
\item sum: $\ve{A} + \ve{B} = [a_{ij} + b_{ij}]_{m \times n}$

\item scalar multiple: $c\ve{A} = [c a_{ij}]_{m \times n}$

\item multiplication: $\ve{C} = \ve{A}\ve{B}$;

$\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times p}$, and $\ve{C} \in \mathbb{C}^{m \times p}$, then $c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}$

$\ve{A}\ve{B} \neq \ve{B}\ve{A}$

\item commutative property: $\ve{A} + \ve{B} = \ve{B} + \ve{A}$

\item associative property: $(\ve{A} + \ve{B}) + \ve{C} = \ve{A} + (\ve{B} + \ve{C})$

\item distributive property: $c(\ve{A} + \ve{B}) = c\ve{A} + c\ve{B}$

\end{enumerate}


%-------------------------------------------------------------
\subsection{Definitions}

Given $\ve{A} \in \mathbb{C}^{m \times n}$, \ve{A} is
%
\begin{enumerate}
\item Transpose: $\ve{A} \in \mathbb{C}^{m \times n}$, and $\ve{B} \in \mathbb{C}^{n \times m} = \ve{A}^T from$

$b_{	ij} = a_{ji}$ for $i = 1, \dots, n$ and $j = 1, \dots, m$

\item Conjugate Transpose / adjoint, $\ve{A}^H = \cc{\ve{A}^T}$ is the complex conjugate of the transpose. 

\item Inverse: $\ve{AA}^{-1} = \ve{A}^{-1}\ve{A} = \ve{I}$, where $\ve{I}$ is a diagonal matrix containing ones on the diagonal.

If this exists, $\ve{A}$ is non-singular / invertible. If the inverse exists, it is unique.

\item Symmetric if $\ve{A} = \ve{A}^T$

\item Antisymmetric / skew-symmetric if $\ve{A}^H = -\ve{A}$

\item Hermitian / self-adjoint if $\ve{A} = \ve{A}^H$

\item Regular if $\ve{A}^{-1}$ exists

\item Unitary if $\ve{A}\ve{A}^H = \ve{I}$

\item Normal if $\ve{A}\ve{A}^H = \ve{A}^H\ve{A}$
\end{enumerate}


%-------------------------------------------------------------
%-------------------------------------------------------------
\subsection{Equations and Special Matrices}

We often write systems of equations as $\ve{A}\vec{x} = \vec{b}$ from
\begin{align}
&a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n = b_1 \nonumber \\
&\vdots \nonumber \\
&a_{n1} x_1 + a_{n2} x_2 + \dots + a_{nn} x_n = b_n \nonumber
\end{align}

To find $\vec{x}$, we need to find a way to affect $\ve{A}^{-1}\vec{b}$. This can be done many many ways, and the first thing we'll talk about are methods for inverting the matrix.

\begin{itemize}
\item Tridiagonal matrix has entries on only the main, upper, and lower diagonal
\item Lower triangular has entries on the diagonal and below
\item Upper triangular has entries on the diagonal and above 
\item Block Tridagonal has blocks of elements (like sub-matrices) on the diagonal. The blocks may be full or only partially full. The blocks look like $\ve{D}_k = [\ve{D_{ij}}]_k$
\end{itemize}

The inverse of a diagonal is simply: $d_{ii}^{-1} = 1/d_{ii}$, another diagonal matrix

\textbf{Theorem}: The following are equivalent (see Math 54 or a textbook for proof)
%
\begin{enumerate}
\item $\ve{A}$ is regular
\item Rank($\ve{A}$) = n
\item $\ve{A}\vec{x} = 0$ iff $x=0$
\item $\ve{A}\vec{x} = \vec{b}$ is uniquely solveable $\forall \vec{b}$
\item det($\ve{A}) \neq 0$
\end{enumerate}



%-------------------------------------------------------------
\subsection{Minors, Cofactors, Determinants}

Good illustration: http://www.mathsisfun.com/algebra/matrix-inverse-minors-cofactors-adjugate.html

%If $\ve{A}$ is an $m \times n$ matrix and $k$ is an integer such that $0 < k \leq m$ and $k \leq n$ then a \textbf{$k \times k$ minor}, $M_{kk}$ of $\ve{A}$ is the determinant of the $k \times k$ matrix formed by deleting $m-k$ rows and $n-k$ columns. 

For a square matrix, the \textbf{first order minor}, $M_{ij}$ just deletes the $i^{th}$ row and $j^{th}$ column and takes the determinant. E.g.
%
\begin{align}
    \ve{A} &= \begin{pmatrix}
        1 & 4 & 7 \\
        3 & 0 & 5 \\
        -1 & 9 & 11 \\
    \end{pmatrix} 
    \qquad
    &M_{23} = \text{det}\begin{pmatrix}
       1 & 4 \\
       -1 & 9 
    \end{pmatrix}   
    = ((1 \times 9) - (4 \times -1)) = 13 \nonumber
\end{align} 

The corresponding $i,j$ \textbf{cofactor} of $\ve{A}$ is
%
\begin{equation}
C_{ij} = (-1)^{i+j} M_{ij} \nonumber
\end{equation}
%
You can compute minors and cofactors for all of $\ve{A}$. The $n \times n$ matrix containing all of the cofactors is denoted $\ve{C}$ in this context.

Using these terms, the determinant can be defined in terms of the Laplace expansion
%
\begin{equation}
\text{det}(\ve{A}) = \sum_{j=1}^n a_{ij} C_{ij} = \sum_{i=1}^n a_{ij} C_{ij} \nonumber
\end{equation}

For the above matrix, lets look at the laplace expansion along the second column ($j = 2$; sum runs over $i$)
\begin{align}
\text{det}(\ve{A}) &= (-1)^{1+2} a_{12} M_{12} + (-1)^{2+2} a_{22} M_{22} + (-1)^{3+2} a_{32} M_{32} \nonumber \\
%
&= (-1)^{1+2} \cdot 4 \cdot \text{det}\begin{pmatrix}
        3 & 5 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{2+2} \cdot 0 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        -1 & 11 \\ \end{pmatrix} + (-1)^{3+2} \cdot 9 \cdot \text{det}\begin{pmatrix} 
        1 & 7 \\
        3 & 5 \\\end{pmatrix} \nonumber \\
%
&= -4 \cdot ((3 \cdot 11) - (5 \cdot -1)) + 0 -9 \cdot ((1 \cdot 5) - (7 \cdot 3)) = -8 \nonumber
\end{align}


\textbf{Properties of Determinants}
\begin{itemize}
\item det($\alpha\ve{A}$) = $\alpha^N$ det($\ve{A}$)

\item det($\ve{A}^T$) = det($\ve{A}$)

\item det($\ve{A}^{-1}$) = 1/det($\ve{A}$)

\item det($\ve{AB}$) = det($\ve{A}$)det($\ve{B}$)

\item det($\ve{A}^n$) = [det($\ve{A}$)]$^n$

\item in general, det($\ve{A + B}$) $\neq$ det($\ve{A}$) + det($\ve{B}$) 
\end{itemize}

The inverse of $\ve{A}$ can be obtained from the determinant and cofactor matrix:
%
\begin{equation}
\ve{A}^{-1} = \frac{1}{\text{det}(\ve{A})}\ve{C}^T \nonumber
\end{equation}
%
If the determinant is zero, then the matrix is singular.

A real matrix where $\ve{A}^{-1} = \ve{A}^T$ is called \textbf{orthogonal}


%-------------------------------------------------------------
\section{Norms and Convergence}
\subsection{Vector Norms}
Given $\vec{x}, \vec{y} \in \mathcal{R}^n$, a vector norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\vec{x}|| > 0$ and $||\vec{x}|| = 0$ iff $\vec{x} = 0$ (positive definite)
\item $||\vec{x} + \vec{y}|| \leq ||\vec{x}|| + ||\vec{y}||$ (triangle inequality)
\item $||\alpha \vec{x}|| = |\alpha| ||\vec{x}||$ (homogeneous)
\end{enumerate}

The p-norm:
%
\begin{equation}
||\vec{x}||_p \equiv (|x_1|^p + |x_2|^p + \dots + |x_n|^p)^{1/p} \qquad p \geq 1 \nonumber
\end{equation}
%
\begin{itemize}
\item $||\vec{x}||_1 \equiv |x_1| + |x_2| + \dots + |x_n|$
\item $||\vec{x}||_2 \equiv (|x_1|^2 + |x_2|^2 + \dots + |x_n|^2)^{1/2}$
\item $||\vec{x}||_{\infty} \equiv \displaystyle \max_{1 \leq i \leq n} |x_1|$
\end{itemize}

------------------------------------------------------------------\\
Does anyone have a preference for a first midterm date of Monday, March 17 vs.\ Wednesday, March 19? Straw poll... The second midterm will be in either the last week or second-to-last week of April. Likely the second-to-last.\\
------------------------------------------------------------------ 

\subsection{Inner Products}
Inner products are symmetric and bilinear form in $\mathcal{R}^n$:
%
\begin{itemize}
\item bi-linear (depends on two arguments) $< \vec{x}, \vec{y} >$
\item symmetric $< \vec{x}, \vec{y} > = < \vec{y}, \vec{x} >$
\item linear $< \vec{x} + \alpha \vec{z}, \vec{y} > = < \vec{x}, \vec{y} > + \alpha < \vec{y}, \vec{z} >$
\item positive $< \vec{x}, \vec{x} >$ $>$ 0 for $\vec{x} \neq 0$
\end{itemize}

The dot product is also known as the Euclidean inner product. One can define many inner products, but in this class assume we're using the Euclidean inner product (dot product) unless otherwise specified. 

\subsection{Convergence}
We can use the concepts of norms and inner products to develop some relationships and talk about convergence. 

The Cauchy-Schwartz inequality states
%
\begin{equation}
< \vec{x}, \vec{y} > \leq ||\vec{x}||_2 ||\vec{y}||_2 \nonumber
\end{equation}
%
Two norms, denoted $|| \cdot ||_a$ and $|| \cdot ||_b$ here, are defined as being equivalent if there exists $C_1$ and $C_2$ such that $\forall \vec{x} \in \mathcal{R}^n$
%
\begin{equation}
C_1 ||\vec{x}||_a \leq ||\vec{x}||_b \leq C_2 ||\vec{x}||_a \qquad \text{where } C_1, C_2 = f(n) \nonumber
\end{equation}
%
This leads to the theorem that in $\mathcal{R}^n$ all norms are equivalent (offered without proof). E.g.:
\begin{align}
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_2 \leq \sqrt[]{n} ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_{\infty} \leq ||\vec{x}||_1 \leq n ||\vec{x}||_{\infty} \nonumber \\
%
& ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt[]{n} ||\vec{x}||_2 \nonumber
\end{align}

The inequalities above may not be `sharp'. A sharp inequality is when there could be no better inequality when making the comparison. E.g., when comparing two real numbers/expressions, an inequality is sharp because we could not increase the left or decrease the right by a positive factor and still have it be true ($2 \leq 2$). 

Example: let's show the last item. We will use the Signum function, which extracts the sign of a real number
\begin{align}
              & -1 \text{ if } x < 0 \nonumber \\
\text{sign}(x) \equiv &  0 \text{ if } x = 0 \nonumber \\
              &  1 \text{ if } x > 0 \nonumber
\end{align}
%
Let's define $v_i$ as sign$(x_i)$. Then
%
\begin{align}
&< \vec{x}, \vec{v} > = \sum_i x_i v_i = \sum_i x_i \text{sign}(x_i) = \sum_i |x_i| \nonumber \\
%
&\text{but by the Cauchy-Schwartz inequality,}\nonumber \\
%
&< \vec{x}, \vec{v} > \leq ||\vec{x}||_2 ||\vec{v}||_2 \nonumber \\
%
&\text{and we can give an upper bound of } ||\vec{v}||_2 \leq \sqrt[]{n} \nonumber \\ 
%
&\text{thus } ||\vec{x}||_2 \leq ||\vec{x}||_1 \leq \sqrt[]{n} ||\vec{x}||_2 \nonumber
\end{align}

Norm equivalence is very important because if we can prove some property (usually convergence or an error bound) in \textbf{some} norm, we have effectively proven it in \textbf{all} norms. 

\subsubsection{Error}
For $\vec{x}$ (the real solution) and $\hat{x}$ (representing our numerical solution) $\in \mathcal{R}^n$ and some norm p we define
%
\begin{itemize}
\item Absolute error: $||\hat{x} - \vec{x}||_p$
\item Relative error: $||\hat{x} - \vec{x}||_p / ||\vec{x}||$, where $\vec{x} \neq 0$
\end{itemize}

\subsubsection{Convergence}
Given a sequence $\lbrace x^{(k)} \rbrace_{k=1,2,\dots,\infty}$ and some norm, we say that $\lbrace x^{(k)} \rbrace$ converges to $\vec{x}$ if
%
\begin{equation}
\displaystyle\text{lim}_{k \rightarrow \infty} ||x^{(k)} - \vec{x}|| = 0 \nonumber
\end{equation}

\subsection{Matrix Norms}
Given $\ve{A}, \ve{B} \in \mathcal{R}^{m \times n}$, a matrix norm, denoted by $|| \cdot ||$, has the following properties:
%
\begin{enumerate}
\item $||\ve{A}|| > 0$ and $||\ve{A}|| = 0$ iff $\ve{A} = 0$ (positive definite)
\item $||\ve{A} + \ve{B}|| \leq ||\ve{A}|| + ||\ve{B}||$ (triangle inequality)
\item $||\alpha \ve{A}|| = |\alpha| ||\ve{A}||$ (homogeneous)
\end{enumerate}

E.g., the Frobenius norm 
%
\begin{equation}
||\ve{A}||_F = ( \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2 )^2 \nonumber
\end{equation}

\textbf{Definitions}:
\begin{itemize}
\item submultiplicative if $||\ve{A} \ve{B}|| \leq ||\ve{A}|| ||\ve{B}||$

\item Not all norms are submultiplicative, we will only deal with those that are. 

\item subordinate matrix norm for $\ve{A} \in \mathcal{R}_{m \times n}$ and $\vec{x} \in \mathcal{R}_n$:

$||\ve{A}|| \equiv \displaystyle \sup_{\vec{x} \neq \vec{0}} ||\ve{A}\vec{x}|| / ||\vec{x}||$

\item \textbf{supremum}, or least upper bound, of a set S of real numbers is denoted by sup S and is defined to be the smallest real number that is greater than or equal to every number in S.
\end{itemize}

\textbf{examples:}
%
\begin{align}
||\ve{A}||_{1} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{1}}{||\vec{x}||_{1}} =
\displaystyle \max_{1 \leq j \leq n} \sum_{j=1}^m |a_{ij}| \qquad \text{max absolute col sum} \nonumber \\
%
||\ve{A}||_{\infty} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{\infty}}{||\vec{x}||_{\infty}} = 
\displaystyle \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}| \qquad \text{max absolute row sum}\nonumber \\
%
||\ve{A}||_{2} &= \displaystyle \sup_{\vec{x} \neq \vec{0}} \frac{||\ve{A}\vec{x}||_{2}}{||\vec{x}||_{2}} = \text{ the spectral radius of }\ve{A} \nonumber 
\end{align}

The equivalence of norms we talked about with vectors also holds here. Some of the relationships are
%
\begin{align}
||\ve{A}||_{2} &\leq ||\ve{A}||_{F} \leq \sqrt{n}||\ve{A}||_{2} \nonumber \\
%
\frac{1}{\sqrt{n}}||\ve{A}||_{\infty} &\leq ||\ve{A}||_{2} \leq \sqrt{m}||\ve{A}||_{\infty} \nonumber \\
%
\frac{1}{\sqrt{m}}||\ve{A}||_{1} &\leq ||\ve{A}||_{2} \leq \sqrt{n}||\ve{A}||_{2} \nonumber
\end{align}

Again, as with vector norms, showing a property in some matrix norms implies that property in other matrix norms (which may be harder to compute). The inequalities above may not be `sharp'.

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------
\section{Eigenvalues}

Recall $\ve{A} \vec{u} = \lambda \vec{u}$ and thus $(\ve{A} - \lambda \ve{I}) \vec{u}=0$ given a non-zero $\vec{u}$.

The spectrum of eigenvalues of a matrix $\ve{A}$ are defined formally as
\[\sigma(\ve{A}) = [ \lambda \in \mathcal{C} : \det(\ve{A} - \lambda \ve{I})=0] \] 
and an eigenvalue is 
\[ \lambda \in \sigma(\ve{A}) \]
%
\begin{itemize}
\item The `characteristic polynomial' of $\ve{A}$ is
\[P(\lambda) = \det(\ve{A} - \lambda \ve{I}) = 0\]
\item Eigenvalues may be real or complex. 
\item Since an \nth degree polynomial has $n$ roots, $\ve{A}_{n \times n}$ is guaranteed to have $n$ real and/or complex eigenvalues, some of which may be repeated: $\lambda_1, \lambda_2, \dots, \lambda_n$.

This gives $\ve{A}\vec{u}_1 = \lambda_1 \vec{u}_1$, etc.
\item $\sigma(\ve{A}) = \sigma(\ve{A}^T)$
\item $\cc{\sigma(\ve{A})} = \sigma(\ve{A}^H)$
\end{itemize}

%--------------------------------------------------------------------------
\subsection{Spectral radius} 

The spectral radius of $\ve{A}$ is defined as 
\[\rho(\ve{A}) \equiv \max \lbrace |\lambda|, \lambda \in \sigma(\ve{A})\]
With the properties:
%
\begin{itemize}
\item $\rho(\ve{A})^k = \rho(\ve{A}^k)$
\item $\rho(\ve{A}) = \rho(\ve{A}^T)$
\item $\rho(\xi \ve{A})^k = |\xi| \rho(\ve{A})$
\end{itemize}

%--------------------------------------------------------------------------
%\subsection{Information}
%The eigenvalues of a matrix satisfy $\ve{A}x = \lambda x$. 
%%
%\begin{itemize}
%\item An eigenvector is a non-zero vector $x$ that, when multiplied by the matrix \ve{A}, yields a constant multiple of $x$. 
%\item The constant multiple, $\lambda$, is the eigenvalue corresponding to the eigenvector.
%\item There can be (and usually are) more than one eigenvalue, and more than one eigenvector. 
%%\item Several eigenvectors can have the same eigenvalue
%\item Sometime the equation can be reformulated as as $y\ve{A} = \alpha y$ (the left eigenvector). In nuclear, we're used to seeing the right eigenvector formulation.
%\end{itemize}


\textbf{example}
Show that the eigenvalues of a Hermitian matrix are real, noting that the eigenvalues of $\ve{A}$ satisfy
\[\ve{A}\vec{x} = \lambda x \qquad \text{and } \vec{x} \neq \vec{0}\]
and recall that $\ve{A} = \ve{A}^H = \ccm{A}^T$ defines a Hermitian matrix.
%
\begin{align}
\cc{u}^T \ccm{A}^T &= \cc{\lambda} \cc{u}^T
\qquad \text{The first step} \nonumber \\
%
\cc{u}^T \ve{A} &= \cc{\lambda} \cc{u}^T 
\qquad \text{apply Hermitian definition}\nonumber \\
%
\cc{u}^T \ve{A} \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
\qquad \text{right multiply by }\vec{u}\nonumber \\
%
\cc{u}^T \lambda \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
\qquad \text{apply eigenvalue definition }\nonumber \\
%
\lambda &= \cc{\lambda} 
\qquad \text{recognize that the }\vec{u}\text{s factor out}\nonumber
\end{align}
 
 %--------------------------------------------------------------------------
\textbf{All matrix norms are bounded}: for $\lambda \in \sigma(\ve{A})$ 
\[|\lambda | \leq ||\ve{A}||\]
\[\rho(\ve{A}) \leq ||\ve{A}||\]

\underline{proof:} 
\begin{enumerate}
\item The eigenvalues of $\ve{A}$ satisfy $\ve{A}\vec{x} = \lambda x$, $\vec{x} \neq \vec{0}$. 

\item Let the eigenvectors of $\ve{A}$ be $\vec{v} = [\vec{v_1}, \vec{v_2}, \dots, \vec{v_n}]$

\item then 
\begin{align}
|\lambda | ||\vec{v}|| &= ||\lambda \vec{v}|| 
\qquad \text{property of norms} \nonumber \\
%
                     &= ||\ve{A}\vec{v}|| 
                     \qquad \text{substitution} \nonumber \\
%
                     &\leq ||\ve{A}|| ||\vec{v}||
                     \qquad \text{triangle inequality} \nonumber \\
%
\therefore |\lambda | &\leq ||\ve{A}|| 
\qquad \text{cancellation}\nonumber \\
%
|\lambda| &\leq \rho(\ve{A}) \leq ||\ve{A}|| 
\qquad \text{definition of }\rho\text{ as max }|\lambda | \nonumber
\end{align}
\end{enumerate}

Note, we can use any submultiplicative matrix norm to bound from above the spectral radius (This will be important).



%\subsubsection{LU Decomposition}
%We can often decompose $\ve{A}$ into an upper and lower triangular matrix
%%
%\begin{align}
%\ve{A} &= \ve{L}\ve{U} \qquad \text{and then} \nonumber \\
%%
%\text{det}(\ve{A}) &= \text{det}(\ve{L}) \text{det}(\ve{U}) \qquad \text{and} \nonumber \\
%%
%\ve{A}^{-1} &= \ve{U}^{-1}\ve{L}^{-1} \nonumber
%\end{align}



\end{document}