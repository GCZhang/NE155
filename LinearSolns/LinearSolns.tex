%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Classes 8 \& 9 \& 10, S14 \\
Solutions for Linear Systems \\ February 7 and 10 and 12, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

%--------------------------------------------------------------------
\section{Solutions of Linear Systems}

Given $\ve{A}\vec{x} = \vec{b}$, how do you find $\vec{x}$?
%
\begin{enumerate}
\item Direct Methods
  \begin{itemize}
  \item compute (explicitly or implicitly) $\ve{A}^{-1} \rightarrow \vec{x} = \ve{A}^{-1}\vec{b}$
  \item typically requires many operations (FLOPs), large storage (even if $\ve{A}$ is sparse, $\ve{A}^{-1}$ may not be)
  \item seldom parallelizes well
  \item \emph{not} used in practice
  \item incomplete factorizations (which we'll cover) \emph{are} used as pre-conditioners for iterative methods
  \end{itemize}
  
\item Iterative Methods
  \begin{itemize}
  \item produce a sequence of vectors, $\vec{x}^{(1)}, \vec{x}^{(2)}, \dots$ based on the prescription
  \[\vec{x}^{(k+1)} = F(\vec{x}^{(k)}, \vec{b})\:, \qquad \text{where } \displaystyle \lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{x}\]
  \item can require many FLOPs; storage requirements for $\ve	{A}$ can be an issue (if stored)
  \item may parallelize well
  \item \emph{often} used in practice
  \item used in smoothing methods and as pre-conditioners
  \item Jacobi, Gauss-Seidel, SOR, Multigrid, ...
  \end{itemize}
  
\item Semi-Iterative Methods
  \begin{itemize}
    \item produce a sequence of vectors, $\vec{x}^{(1)}, \vec{x}^{(2)}, \dots$ based on the prescription
  \[\vec{x}^{(k+1)} = F(\vec{x}^{(k)}, \vec{x}^{(k-1)}, \dots, \vec{x}^{(0)}, \vec{b})\:, \qquad \text{where } \displaystyle \lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{x}\]  
  The function does not necessarily have to use all previous $\vec{x}$ iterates.
  \item can require many FLOPs; storage requirements for $\ve	{A}$ can be an issue (if stored)
  \item may parallelize well
  \item Commonly used in conjugate gradients, generalized minimal residuals methods
  \end{itemize}
\end{enumerate}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\section{Direct Methods}

\subsection{Diagonal Systems}
\begin{equation}
   \begin{pmatrix}
      a_{11} & 0      & \cdots & 0      & \cdots & 0 \\
      0      & a_{22} & \cdots & 0      & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\     
      0      & 0      & \cdots & a_{ii} & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & 0      & \cdots & a_{nn} 
    \end{pmatrix} 
    \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n \end{pmatrix} =
    \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_j \\ \vdots \\ b_n \end{pmatrix}
    \nonumber   
\end{equation} 
%
To solve this, simply
%
\[x_i = \frac{b_i}{a_{ii}} \:, i=1,\dots,n\:.\]
%
\begin{itemize}
\item this requires $O(n)$ operations.
\item if $a_{ii}=0$, then $\det(\ve{A})=0$ (the determinant of a diagonal matrix is the product of its diagonal entries) and the system does not have a solution.
\item If, however, $b_i = 0$ for the $a_{ii}$ is question, then the system has an infinite number of solutions.
\end{itemize}

%--------------------------------------------------------------------
\subsection{Lower/Upper Triangular Systems}
\begin{equation}
   \begin{pmatrix}
      a_{11} & 0      & \cdots & 0      & \cdots & 0 \\
      a_{21} & a_{22} & \cdots & 0      & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\     
      a_{i1} & a_{i2} & \cdots & a_{ii} & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
      a_{n1} & a_{n2} & \cdots & a_{ni} & \cdots & a_{nn} 
    \end{pmatrix} 
    \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n \end{pmatrix} =
    \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_j \\ \vdots \\ b_n \end{pmatrix}
    \nonumber   
\end{equation} 
%
To solve this, solve the first equation for the one unknown, which becomes know. Then the second equation for the (now) one unknown, etc. This is called forward substitution.
%
\begin{align}
&x_1 = \frac{b_1}{a_{11}} \nonumber \\
&\text{for } i = 2, n \nonumber \\
&\qquad x_i = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j) \nonumber
\end{align}

%--------------------------------------------------------------------
Upper triangular systems work the same way, but you go in the reverse order $\rightarrow$ backward substitution. 
%
\begin{align}
&x_n = \frac{b_n}{a_{nn}} \nonumber \\
&\text{for } i = n-1, -1, 1 \nonumber \\
&\qquad x_i = \frac{1}{a_{ii}}(b_i - \sum_{j=i+1}^{n} a_{ij} x_j) \nonumber
\end{align}

These both require $O(n^2)$ operations, and each have the same caveats about zeros as the diagonal system.

%--------------------------------------------------------------------
\subsection{LU Decomposition}

\textbf{Theorem}

Let $\ve{A} \in \mathcal{R}^{m \times n}$ be regular (recall: regular means $\ve{A}^{-1}$ exists). Then there are matrices $\ve{L}$ (lower triangular) with $l_{ii} = 1$ and $\ve{U}$ (upper triangular) such that $\ve{A} = \ve{L}\ve{U}$ is unique.

We want to solve $\ve{A}\vec{x} = \vec{b}$:
%
\begin{align}
&\ve{A}\vec{x} = \ve{L}\ve{U}\vec{x} = \vec{b} \nonumber \\
%
&\text{define } \ve{U}\vec{x} = \vec{y} \nonumber \\
%
&\text{then note }\ve{L}\vec{y} = \vec{b} \nonumber
\end{align}
%
\begin{enumerate}
\item Solve $\ve{L}\vec{y} = \vec{b}$ for $\vec{y}$ using forward substitution
\item Solve $\ve{U}\vec{x} = \vec{y}$ for $\vec{x}$ using backward substitution
\end{enumerate}
%
This requires $O(n^2)$ operations.

The \textbf{LU decomposition} is formed by Gaussian Elimination, requiring $O(n^3)$ operations. $\ve{U}$ is the upper triangular matrix from Gaussian Elimination; $\ve{L}$ is the lower triangular matrix (formed from performing the same operations on $\ve{I}$ that you performed on $\ve{U}$). 
%
\begin{equation}
   \ve{L} = \begin{pmatrix}
      1      & 0      & \cdots    & 0 \\
      m_{21} & 1      & \cdots    & 0 \\
      \vdots & \vdots & \ddots    & \vdots \\     
      m_{n1} & \cdots & m_{n,n-1} & 1 
    \end{pmatrix} \qquad
  \ve{U} = \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      0      & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\     
      0      & 0      & \cdots &  a_{nn} 
    \end{pmatrix}
    \nonumber   
\end{equation} 
%
For a given column, $j = 1, \dots, n$, the $m_{ij}$ are $a_{ij} / a_{jj}$ for $i = j, \dots, n$. 

Caveat: we use the entries currently in $\ve{A}$ that come from the process of row-reducing $\ve{A}$ to get $\ve{U}$. This will be made clearer through an example:

\textbf{example}
\begin{equation}
\ve{A} = \begin{pmatrix}
  5  & 2 & 1 \\
  4  & 1 & -1 \\
  -2 & 3 & -3
\end{pmatrix}
\qquad 
\vec{b}= \begin{pmatrix}
3 \\ -3 \\ 5
\end{pmatrix} \nonumber
\end{equation}
%
%
Create $\ve{U}$ by row-reducing $\ve{A}$ to echelon form. 
\begin{align}
\ve{U} &= \begin{pmatrix}
  5  & 2 & 1 \\
  4  & 1 & -1 \\
  -2 & 3 & -3
\end{pmatrix} 
&\begin{pmatrix}
 & \\ R_2 - m_{21}R_1, & m_{21} = 4/5 \\ 
      R_3 - m_{31}R_1, & m_{31} = -2/5 \end{pmatrix} \nonumber \\
%
&= \begin{pmatrix}
  5 & 2    & 1 \\
  0 & -3/5 & -9/5 \\
  0 & 19/5 & -13/5
\end{pmatrix} 
&\begin{pmatrix}
 & \\ & \\ R_3 - m_{32}R_2, & m_{32} = -19/3 \end{pmatrix} \nonumber \\                   
%
&= \begin{pmatrix}
  5 & 2    & 1 \\
  0 & -3/5 & -9/5 \\
  0 & 0    & -14
\end{pmatrix} \nonumber
\end{align}
%
Now, we fill the $m_{ij}$s we just discovered into the $\ve{L}$ matrix.
%
\begin{equation}
\ve{L} = \begin{pmatrix}
  1    & 0     & 0 \\
  4/5  & 1     & 0 \\
  -2/5 & -19/3 & 1
\end{pmatrix} \nonumber
\end{equation}

Using all of this, we can finally solve our equation. Begin with $\ve{L}\vec{y} = \vec{b}$ for $\vec{y}$ using forward substitution:
%
\begin{equation}
\ve{L} = \begin{pmatrix}
  1    & 0     & 0 \\
  4/5  & 1     & 0 \\
  -2/5 & -19/3 & 1
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2 \\ y_3
\end{pmatrix}
= \begin{pmatrix}
3 \\ -3 \\ 5
\end{pmatrix} \nonumber
\end{equation}
%
\begin{align}
&\boxed{y_1 = 3} \nonumber \\
&(\frac{4}{5})(3) + y_2 = -3 \rightarrow \boxed{y_2 = -\frac{27}{5}} \nonumber \\
&(-\frac{2}{5})(3) + (-\frac{19}{3})(-\frac{27}{5}) + y_3 = 5
\rightarrow \boxed{y_3 = -28} \nonumber
\end{align}

Now, we solve $\ve{U}\vec{x} = \vec{y}$ for $\vec{x}$
%
\begin{equation}
\ve{U} = \begin{pmatrix}
  5 & 2    & 1 \\
  0 & -3/5 & -9/5 \\
  0 & 0    & -14
\end{pmatrix} 
\begin{pmatrix}
x_1 \\ x_2 \\ x_3
\end{pmatrix}
= \begin{pmatrix}
3 \\ -\frac{27}{5} \\ -28
\end{pmatrix}\nonumber
\end{equation}
%
\begin{align}
&-14 x_3 = -28 \rightarrow \boxed{x_3 = 2} \nonumber \\
&(-\frac{3}{5})x_2 + (-\frac{9}{5})(2) = -\frac{27}{5} \rightarrow \boxed{x_2 = 3} \nonumber \\
&(5)x_1 + (2)(3) + (1)(2) = 3
\rightarrow \boxed{x_1 = -1} \nonumber
\end{align}

\begin{itemize}
\item The LU decomposition and solution is exact
\item given the LU decomposition, multiple solution vectors can be computed for a small cost
\item the LU decomposition can overwrite the original matrix, if desired
\item the LU decomposition requires $O(n^3)$ FLOPS
\item the LU decomposition `fills' a sparse matrix
\end{itemize}	

\subsection{Direct Factorization Algorithm}
Begin with $n \times n$ matrices $\ve{U}$ and $\ve{L}$ filled with zeros.
%
\begin{enumerate}
\item For $i = 1, \dots, n$: $l_{ii} = 1$
\item $u_{11} = a_{11}$

\item For $j = 2, \dots, n$\\
  \hspace*{1 em} $u_{1j} = a_{1j}$\\
  \hspace*{1 em} $l_{j1} = a_{j1}/u_{11}$
 
\item For $i = 2, \dots, n-1$\\
  \hspace*{1 em} $u_{ii} = a_{ii} - \sum_{k=1}^{i-1} l_{ik} u_{ki}$\\
  \hspace*{1 em} For $j = i+1, \dots, n$\\   
    \hspace*{3 em} $u_{ij} = a_{ij} - \sum_{k=1}^{i-1} l_{ik} u_{ki}$\\
    \hspace*{3 em} $l_{ji} = 1/u_{ii}[a_{ij} - \sum_{k=1}^{i-1} l_{jk} u_{ki}$
    
\item $u_{nn} = a_{nn} - \sum_{k=1}^{n-1} l_{nk} u_{kn}$
\end{enumerate}

\subsection{Pivoting}
When any of the diagonal coefficients are equal to zero (or close to zero), one way of avoiding it is row or column pivoting (exchange of rows or columns). We keep track of pivoting through a permutation matrix:
\[\ve{P}\ve{A} = \ve{L}\ve{U}\]
where $\ve{P}$ has one entry of unity per column and row.

For example, if we have a situation where $u_{22}$ is very small but $u_{32}$ isn't, we could flip rows 2 and 3. Note: this also requires flipping $b_2$ and $b_3$. In this case, 
\[\ve{P} = \begin{pmatrix}
  1 & 0 & 0 \\
  0 & 0 & 1 \\
  0 & 1 & 0
\end{pmatrix} \]

\subsection{Tridiagonal}
If $\ve{A}$ is tridiagonal, LU decomposition can be streamlined by using the Thomas algorithm:
%
\begin{align}
    \ve{T} &= \begin{pmatrix}
      a_{11} & a_{12} & 0      & \cdots    & 0 \\
      a_{21} & a_{22} & a_{23} & \cdots    & 0 \\
      0      & a_{32} & \ddots & \ddots    & \vdots \\     
      \vdots & \vdots & \ddots & \ddots    & a_{n-1 n}\\
      0      & 0      & \cdots & a_{n n-1} & a_{nn} \\
    \end{pmatrix} =
    \begin{pmatrix}
      d_{1}  & u_{1}  & 0      & \cdots & 0 \\
      l_{2}  & d_{2}  & u_{2}  & \cdots & 0 \\
      0      & l_{3}  & \ddots & \ddots & \vdots \\     
      \vdots & \vdots & \ddots & \ddots & u_{n-1}\\
      0      & 0      & \cdots & l_{n}  & d_{n} \\
    \end{pmatrix} \nonumber   
\end{align} 

\subsubsection{Thomas Algorithm}
\begin{enumerate}
\item For $i=2, \dots, n$, overwrite\\
  \hspace*{1 em} $d_i = d_i - (l_i / d_{i-1}) u_{i-1}$
  \hspace*{1 em} $b_i = b_i - (l_i / d_{i-1}) b_{i-1}$ (forward sub)

\item Get $\vec{x}$ through backward sub:\\
$x_n = b_n / d_n$\\
For $i = n-1, \dots, 1$:\\
  \hspace*{1 em} $x_i = (b_i - u_i x_{i+1}) d_i$
\end{enumerate}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\section{Iterative Methods}
We need iterative methods when our system of linear equations is large. Recall: produce a sequence of vectors, $\vec{x}^{(1)}, \vec{x}^{(2)}, \dots$ based on the prescription
  \[\vec{x}^{(k+1)} = F(\vec{x}^{(k)}, \vec{b})\:, \qquad \text{where } \displaystyle \lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{x}\] 

We will write this as \[\vec{x}^{(k+1)} = \ve{P}\vec{x}^{(k)} +  \tilde{\vec{b}}\:, k = 0, 1, \dots \]

We will split $\ve{A}$ to execute our process.
%
\begin{align}
(\ve{A} + \ve{S}) \vec{x} &= \ve{S}\vec{x} + \vec{b} \nonumber \\
%
\ve{Q} \vec{x} &= \ve{S}\vec{x} + \vec{b} 
\qquad\text{where } \ve{Q} = \ve{A} + \ve{S} \nonumber \\
%
\vec{x} &= \ve{Q}^{-1} \ve{S}\vec{x} + \ve{Q}^{-1} \vec{b} 
\nonumber \\
%
\vec{x} &= \ve{P}\vec{x} + \tilde{\vec{b}}  
\qquad\text{assuming regular } \ve{Q}\nonumber
\end{align}
%
The \textbf{fixed-point} iterative process is:
\begin{align}
\vec{x}^{(0)} &= \text{ arbitrary}\nonumber \\
\vec{x}^{(k+1)} &= \ve{P}\vec{x}^{(k)} + \tilde{\vec{b}} \nonumber
\end{align}

To determine convergence, we define the iterative error as follows, noting that $\vec{x}$ is defines as a solution to $\ve{A}\vec{x} = \vec{b}$. 
%
\begin{align}
\vec{e}^{(k)} &= \vec{x}^{(k)} - \vec{x} \nonumber \\
%
&\text{Subtract the exact solution from the method} \nonumber \\
%
\vec{x}^{(k+1)} &= \ve{P}\vec{x}^{(k)} + \tilde{\vec{b}} \nonumber \\
- (\vec{x} &= \ve{P}\vec{x} + \tilde{\vec{b}}) \nonumber \\
%
\vec{x}^{(k+1)} - \vec{x} &= \ve{P}(\vec{x}^{(k)} - \vec{x})\nonumber \\
%
\text{Thus, } \vec{e}^{(k+1)} &= \ve{P}\vec{e}^{(k)} \nonumber \\
%
\text{Further, } \vec{e}^{(k+1)} &= \ve{P}^2\vec{e}^{(k-1)} = \ve{P}^3\vec{e}^{(k-2)} = \dots = \ve{P}^{k+1}\vec{e}^{(0)}\nonumber
%
\end{align}
%
Now we can use norms  to determine a rate of convergence
\[||\vec{e}^{(k+1)}|| = ||\ve{P}^{k+1}\vec{e}^{(0)}|| \leq ||\ve{P}^{k+1}|| ||\vec{e}^{(0)}|| \]
and our rate of convergence is governed by $||\ve{P}^{k+1}||$

There is a theorem (offered without proof):
\[\lim_{k \rightarrow \infty} || \ve{P}^k ||^{1/k} = \rho	(\ve{P})\]
and this implies
\[|| \ve{P}^k || \approx \rho^k (\ve{P})\]

\underline{Alternative:} Recall that $|| \ve{P}^k ||_2$ = $\rho(\ve{P}^k)$ and that $ \rho^k (\ve{P}) = \rho(\ve{P}^k)$. We also know that all norms can be related, so finding an error bound in one norm bounds all norms. 

Thus, a sufficient and necessary condition for convergence is:
\[\rho^{k+1}(\ve{P}) < 1 \rightarrow \rho(\ve{P}) < 1 \]

\underline{Note:} $\ve{Q}^{-1}$ must be easy to compute. We often select $\ve{Q}$ to be diagonal, triangular, or tri-diagonal. Further, $\ve{Q}$ must be chosen such that $\rho(\ve{P}) < 1$.

%--------------------------------------------------------------------
\subsection{Richardson Iteration / Source Iteration}

In non-stationary Richardson iteration using some scalar $\omega$, the $k$th step is
%
\begin{equation}
  \vec{x}^{(k+1)} = (\ve{I} - \omega^{(k)}\ve{A})\vec{x}^{(k)} + \omega^{(k)}\vec{b} \:. \nonumber
\end{equation}
%
$\ve{P} = (\ve{I} - \omega^{(k)}\ve{A})$. If $\omega^{(k)}$ is constant then this is the stationary Richardson method. 

The algorithm for this method is, for $i = 1, \dots, n$:
\[ x^{(k+1)}_i =  x^{(k)}_i - \omega^{(k)} \sum_{j=1}^{n} a_{ij}x_j^{(k)} + \omega^{(k)} b_i \]

People still use these methods. I've written a preconditioner that uses Richardson Iteration. The spectral radius determines the speed of convergence and is $c = \Sigma_s / \Sigma$. For problems dominated by scattering, SI will converge very slowly \cite{Evans2009d}. While this is the simplest iterative method used, there are other more sophisticated methods available. 

%--------------------------------------------------------------------
\subsection{Jacobi Iteration}

Let $\ve{D} = diag(\ve{A})$, then
\begin{align}
\ve{D} \vec{x}^{k+1} &= (\ve{D} - \ve{A})\vec{x}^{(k)} + \vec{b} \nonumber \\
%
\vec{x}^{k+1} &= \ve{D}^{-1}(\ve{D} - \ve{A})\vec{x}^{(k)} + \ve{D}^{-1}\vec{b} \nonumber
\end{align}
%
In our original syntax, $\ve{P}_J = \ve{I} -  \ve{D}^{-1}\ve{A}$ and $\tilde{\vec{b}} =\ve{D}^{-1}\vec{b}$.

The algorithm for this method is, for $i = 1, \dots, n$:
\[ x^{(k+1)}_i = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)})\]

%Jacobi is unconditionally stable and linearly convergent, but may converge very slowly. 
Jacobi will converge (perhaps quite slowly) if $\ve{P}_J$'s spectral radius is $<$ 1, but that's often difficult to estimate \cite{LeVeque2007}. 

A \underline{sufficient} but not necessary condition for convergence is that $\ve{A}$ or $\ve{A}^T$ is \textbf{diagonally dominant}:
%
\[|a_{ii}| \geq \sum_{j \neq 1} |aij| \qquad \forall i \:.\]

Jacobi isn't a very sophisticated method, but it's easy to parallelize. Why might that be? The Jacobi method is order independent since all terms on the right are at the old iteration level. 


%--------------------------------------------------------------------
\subsection{Gauss Seidel Iteration}

Gauss Seidel is a simple modification of Jacobi, but becomes a bit more complicated. How can you think of changing Jacobi that might incorporate more information that we already have?

The algorithm for this method is, for $i = 1, \dots, n$:
\[ x^{(k+1)}_i = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}) \:.\]
%
In matrix notation, this looks like:
\[(\ve{D} + \ve{L})\vec{x}^{k+1} = -\ve{U} \vec{x}^{(k)} + \vec{b}\:, \]
where we have separated $\ve{A}$ as $\ve{L} + \ve{D} + \ve{U}$. 

This makes $\ve{P}_{GS} = (\ve{D} + \ve{L})^{-1}\ve{U}$. Gauss Seidel %is also unconditionally stable and linearly convergent, and 
converges twice as fast as Jacobi (not proven here). However, both methods may converge very slowly - especially in highly scattering systems \cite{LeVeque2007}. 

The spectral radii for some materials of practical interest determined using cross sections with 41 thermal upscattering groups are: graphite = 0.9984, heavy water = 0.9998, and iron = 0.6120 \cite{Adams2002}, \cite{Evans2009d}.

A \underline{sufficient} but not necessary condition to converge is that $\ve{A}$ be \textbf{symmetric} and \textbf{positive definite}.

A $n \times n$ Hermitian matrix $\ve{A}$ is said to be positive definite if $\vec{z}^H \ve{A} \vec{z}$ is real and positive for all non-zero complex vectors $\vec{z}$. This also simplifies to the pure-real case.

Gauss Seidel is not so easy to parallelize. Why not? The GS method is order dependent, containing terms on the right hand side at both the new and old iterates.


%--------------------------------------------------------------------
\subsection{Successive Over Relaxation (SOR)}

SOR is similar to GS, but now we add a splash of control to try to converge faster than GS (sort of like weighted Richardson). 
%
\[(\ve{D} + \omega \ve{L})\vec{x}^{k+1} = [(1-\omega)\ve{D} - \omega \ve{U}] \vec{x}^{(k)} + \omega\vec{b}\:, \]
%
where $0 < \omega < 2$. Now $\ve{P}_{SOR} = (\ve{D} + \omega\ve{L})^{-1} [(1-\omega)\ve{D} - \omega \ve{U}]$. 

For $i = 1, \dots, n$:
\[ x^{(k+1)}_i = (1-\omega)x_i^{(k)} + \frac{\omega}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}) \:.\]

Whether and how much better we do that GS is determined by $\omega$.
%
\begin{figure}[h!]
\begin{center}
\includegraphics[height=2.25in]{SOR-omega}
\end{center}
\end{figure}
%
Determining the best $\omega$, $\omega_{opt}$ is problem-dependent and is not practical to do in general.

%--------------------------------------------------------------------
\subsection{Number of Iterations?}

How many iterations does it take to reduce error by a factor of $\epsilon$? 
%For an idealized problem with $n$ unknowns and some constant $C_J$, we can say:
%%
%\begin{align}
%\rho(\ve{P}_J) &= 1 - C_J \frac{1}{n^2} + \dots \nonumber \\
%%
%\rho(\ve{P}_{GS}) &= 1 - 2 C_J \frac{1}{n^2} + \dots \nonumber \\
%%
%\rho(\ve{P}_{SOR}, \omega_{opt}) &= 1 - 2\sqrt{2 C_J} \frac{1}{n^2} + \dots \nonumber
%\end{align}
%%
%Then
%
\begin{align}
|| \vec{e}^{(k)} || &\leq \epsilon || \vec{e}^{(0)} || \nonumber \\
%
|| \vec{e}^{(k)} || &\leq \rho(\ve{P})^k || \vec{e}^{(0)} || \nonumber \\
%
\rho(\ve{P})^k &\approx \epsilon 
\qquad \text{take the log of both sides}\nonumber \\
%
k \log(\rho(\ve{P})) &\approx \log(\epsilon)\nonumber \\
%
k &\approx \frac{\log(\epsilon)}{\log(\rho(\ve{P}))} \nonumber
\end{align}

%--------------------------------------------------------------------
\subsection{Preconditioning}
Condition number information is derived from Trefethen and Bau \cite{Trefethen1997}. Conditioning is one way to express the perturbation behavior of the mathematical problem. 

A \emph{well-conditioned} problem is one in which all small perturbations of $x$ lead to only small changes in $f(x)$. 

An \emph{ill-conditioned} problem is one in which some small perturbation of $x$ leads to a large change in $f(x)$. 

The condition number is a quantity used to express how well-conditioned a matrix or problem is. A small condition number corresponds to a well-conditioned problem, and vice versa. 

The \textbf{condition number} of a matrix $\mathbf{A}$ is defined as
\[\kappa(\mathbf{A}) = ||\mathbf{A}|| \text{ }||\mathbf{A}^{-1}|| \:.
\]
%
If the 2-norm, then $||\mathbf{A}||_{2} = \sigma_{1}$, $||\mathbf{A}^{-1}||_{2} = 1 / \sigma_{m}$, and $\kappa_{2}(\mathbf{A}) = \sigma_{1} / \sigma_{m}$; $\sigma_{m}$ is the $m$th singular value of $\ve{A}$. If $\mathbf{A}$ is singular, its condition number is infinity. 

The base-b logarithm of $\kappa$ is an estimate of how many base-b digits are lost in solving a linear system with that matrix. In other words, it estimates worst-case loss of precision. A system is said to be singular if the condition number is infinite, and ill-conditioned if it is too large, where ``too large" means roughly $\log(\kappa) >\sim$ the precision of matrix entries. %http://mathworld.wolfram.com/ConditionNumber.html

The general idea of \textbf{preconditioning} is to transform the system of interest into another equivalent system that has more favorable properties, for example one with a smaller condition number. 

A \textbf{preconditioner} is a matrix that induces such a transformation by improving the spectral properties of the problem being solved. 

Let $\ve{G}$ be a non-singular preconditioner, then \cite{Benzi2002} $\ve{A}\vec{x}=\vec{b}$ can be transformed as 
\[ \ve{G}^{-1}\ve{A}x = \ve{G}^{-1}b \:.\] 

Strictly speaking, Jacobi is ``preconditioned Richardson iteration", with $\ve{G} = \ve{D}$.
%in the following ways \cite{Benzi2002}: 
%%
%\begin{alignat}{3}
%  \ve{G}^{-1}\ve{A}x &= \ve{G}^{-1}b  &  &\text{left preconditioning,} \\
%  \ve{AG}^{-1}y &= b, \qquad  &x = \ve{G}^{-1}y \qquad &\text{right preconditioning, and } \\
%  \ve{G}_{1}^{-1}\ve{AG}_{2}^{-1}y &= \ve{G}_{1}^{-1}b, \qquad \ve{G} = \ve{G}_{1}\ve{G}_{2}, \qquad  &x = \ve{G}_{2}^{-1}y  \qquad &\text{split preconditioning.} 
%\end{alignat}
%
%If $\ve{A}$ and/or $\ve{G}$ are non-normal then each of the preconditioning constructs will likely give different behavior, though they will converge to the same answer because the matrices are similar and therefore have the same eigenvalues  \cite{Benzi2002}. Right preconditioning leaves the right hand side of the equation unaffected and does not change the norm of the residual, which is used for convergence testing in most iterative methods. Right preconditioning is usually preferred over left preconditioning for iterative solvers for this reason \cite{Knoll2004}. A right preconditioner was implemented in this work, and the remaining discussion will be presented in right preconditioner format. 

%The matrix $\ve{A}\ve{G}^{-1}$ is not formed in practice. The preconditioner can be applied by using some method to solve $\ve{G}y=c \to y \approx \ve{G}^{-1}c$, or by otherwise implementing the action of $\ve{G}^{-1}$ without ever explicitly forming and inverting $\ve{G}$. There are two extremes between which all other preconditioners lie: $\ve{G} = \ve{A}$, in which case the solution can be found directly, and $\ve{G} = \ve{I}$, which will have no effect \cite{Benzi2002}, \cite{Trefethen1997}. 

Functionally, a good preconditioner should make the system easier to solve and result in faster convergence. It should also be cheap to construct and apply. %These tend to be competing goals in that the easier a preconditioner is to construct and apply the less it typically does to improve convergence. A preconditioner is considered good if $\ve{A}\ve{G}^{-1}$ is not too far from normal and its eigenvalues are clustered \cite{Trefethen1997}. 

There are many different types of preconditioners, but they can be put into two general categories: \textbf{matrix-based} and \textbf{physics-based}.

\underline{Matrix-based} preconditioners rely entirely on the structure of the matrix $\ve{A}$ regardless of the physics of the problem. That is, these methods do not change when the underlying problem changes. This can be a very useful property because matrix-based methods are then broadly applicable and do not require any understanding of the physical problem \cite{Trefethen1997}. %Extrapolation methods and incomplete factorizations are examples of matrix-based preconditioners \cite{Trefethen1997}.

\underline{Physics-based} preconditioning uses knowledge about the physics of the problem in question to guide the creation of the preconditioner. This means that some methods only work with certain kinds of problems, and that the preconditioners may have to be tailored or adapted for different applications. However, such methods take advantage of knowing something about the problem and can be more effective than matrix-based methods for the range of problems for which they are intended \cite{Trefethen1997}. %Rebalance and synthetic acceleration are examples of physics-based preconditioners \cite{Trefethen1997}.


%--------------------------------------------------------------------
\subsection{Examples}

Let's look at 
%
\begin{equation}
\begin{pmatrix}
  4  &  3 & 0 \\
  3  &  4 & -1 \\
  0  & -1 & 4
\end{pmatrix}
%
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} 
%
= \begin{pmatrix} 24 \\ 30 \\ -24 \end{pmatrix} \nonumber
\end{equation}
%
with both GS and SOR. In  both cases we'll start with $\vec{x}^{(0)} = [1, 1, 1]^T$. The exact solution is $[3, 4, -5]^T$. 

\textbf{GS:} for $i = 1, \dots, n$:
\[ x^{(k+1)}_i = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}) \:.\]
%
\begin{align}
x_1^{(k+1)} &= \frac{1}{4}(24 - 3 x_2^{(k)} - 0 x_3^{(k)}) 
= 6 - \frac{3}{4}x_2^{(k)} \nonumber \\
%
x_2^{(k+1)} &= \frac{1}{4}(30 - 3 x_1^{(k+1)} - (-1) x_3^{(k)}) 
= \frac{30}{4} - \frac{3}{4}x_1^{(k+1)} + \frac{1}{4}x_3^{(k)}\nonumber \\
%
x_3^{(k+1)} &= \frac{1}{4}(-24 - 0 x_1^{(k+1)} - (-1) x_2^{(k+1)}) 
= -6 + \frac{1}{4}x_2^{(k+1)}\nonumber
\end{align}

\textbf{SOR:} for $i = 1, \dots, n$:
\[ x^{(k+1)}_i = (1-\omega)x_i^{(k)} + \frac{\omega}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}) \:.\]
%
Let $\omega = 1.25 = 5/4$
%
\begin{align}
x_1^{(k+1)} &= (1-\frac{5}{4})x_1^{(k)} + \frac{\frac{5}{4}}{4}(24 - 3 x_2^{(k)} - 0 x_3^{(k)}) 
= \frac{15}{2} - \frac{1}{4}x_1^{(k)} - \frac{15}{16}x_2^{(k)} \nonumber \\
%
x_2^{(k+1)} &= (1-\frac{5}{4})x_2^{(k)} + \frac{\frac{5}{4}}{4}(30 - 3 x_1^{(k+1)} - (-1) x_3^{(k)}) 
= \frac{150}{16} - \frac{15}{16}x_1^{(k+1)} - \frac{1}{4}x_2^{(k)} + \frac{5}{16}x_3^{(k)}\nonumber \\
%
x_3^{(k+1)} &= (1-\frac{5}{4})x_3^{(k)} + \frac{\frac{5}{4}}{4}(-24 - 0 x_1^{(k+1)} - (-1) x_2^{(k+1)}) 
= -\frac{15}{2} + \frac{5}{16}x_2^{(k+1)} - \frac{1}{4}x_3^{(k)}\nonumber
\end{align}

\begin{table}[h]
  \caption{Gauss Seidel}
  \centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
k & 0 & 1 & 2 & 3 & 4 & $|$error$|$ \\ \hline
$x_1$ & 1 & 5.25 & 3.140625 & 3.087891 & 3.05493 & 0.05493\\ \hline 
$x_2$ & 1 & 3.8125 & 3.8828125 & 3.92676 & 3.95422 & 0.04578 \\ \hline 
$x_3$ & 1 & -5.046875 & -5.0292969 & -5.01831 & -5.00715 & 0.00715 \\ \hline 
\end{tabular}

\vspace*{1 em}
\caption{SOR}
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
k & 0 & 1 & 2 & 3 & 4 & $|$error$|$ \\ \hline
$x_1$ & 1 & 6.3125 & 2.6223 & 3.1333 & 2.95705 & 0.04295 \\ \hline 
$x_2$ & 1 & 3.51953 & 3.95853 & 4.01026 & 4.00748 & 0.00748 \\ \hline 
$x_3$ & 1 & -6.65015 & -4.60042 & -5.096686 & -4.97349 & 0.02651 \\ \hline 
\end{tabular}
\end{table}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{LinearSolns} 

\end{document}