%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Classes 8 \& 9, S14 \\
Solutions for Linear Systems \\ February 7 and 10, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

%--------------------------------------------------------------------
\section{Solutions of Linear Systems}

Given $\ve{A}\vec{x} = \vec{b}$, how do you find $\vec{x}$?
%
\begin{enumerate}
\item Direct Methods
  \begin{itemize}
  \item compute (explicitly or implicitly) $\ve{A}^{-1} \rightarrow \vec{x} = \ve{A}^{-1}\vec{b}$
  \item typically requires many operations (FLOPs), large storage (even if $\ve{A}$ is sparse, $\ve{A}^{-1}$ may not be)
  \item seldom parallelizes well
  \item \emph{not} used in practice
  \item incomplete factorizations (which we'll cover) \emph{are} used as pre-conditioners for iterative methods
  \end{itemize}
  
\item Iterative Methods
  \begin{itemize}
  \item produce a sequence of vectors, $\vec{x}^{(1)}, \vec{x}^{(2)}, \dots$ based on the prescription
  \[\vec{x}^{(k+1)} = F(\vec{x}^{(k)}, \vec{b})\:, \qquad \text{where } \displaystyle \lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{x}\]
  \item can require many FLOPs; storage requirements for $\ve	{A}$ can be an issue (if stored)
  \item may parallelize well
  \item \emph{often} used in practice
  \item used in smoothing methods and as pre-conditioners
  \item Jacobi, Gauss-Seidel, SOR, Multigrid, ...
  \end{itemize}
  
\item Semi-Iterative Methods
  \begin{itemize}
    \item produce a sequence of vectors, $\vec{x}^{(1)}, \vec{x}^{(2)}, \dots$ based on the prescription
  \[\vec{x}^{(k+1)} = F(\vec{x}^{(k)}, \vec{x}^{(k-1)}, \dots, \vec{x}^{(0)}, \vec{b})\:, \qquad \text{where } \displaystyle \lim_{k \rightarrow \infty} \vec{x}^{(k)} = \vec{x}\]  
  The function does not necessarily have to use all previous $\vec{x}$ iterates.
  \item can require many FLOPs; storage requirements for $\ve	{A}$ can be an issue (if stored)
  \item may parallelize well
  \item Commonly used in conjugate gradients, generalized minimal residuals methods
  \end{itemize}
\end{enumerate}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\section{Direct Methods}

\subsection{Diagonal Systems}
\begin{equation}
   \begin{pmatrix}
      a_{11} & 0      & \cdots & 0      & \cdots & 0 \\
      0      & a_{22} & \cdots & 0      & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\     
      0      & 0      & \cdots & a_{ii} & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & 0      & \cdots & a_{nn} 
    \end{pmatrix} 
    \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n \end{pmatrix} =
    \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_j \\ \vdots \\ b_n \end{pmatrix}
    \nonumber   
\end{equation} 
%
To solve this, simply
%
\[x_i = \frac{b_i}{a_{ii}} \:, i=1,\dots,n\:.\]
%
\begin{itemize}
\item this requires $O(n)$ operations.
\item if $a_{ii}=0$, then $\det(\ve{A})=0$ (the determinant of a diagonal matrix is the product of its diagonal entries) and the system does not have a solution.
\item If, however, $b_i = 0$ for the $a_{ii}$ is question, then the system has an infinite number of solutions.
\end{itemize}

%--------------------------------------------------------------------
\subsection{Lower/Upper Triangular Systems}
\begin{equation}
   \begin{pmatrix}
      a_{11} & 0      & \cdots & 0      & \cdots & 0 \\
      a_{21} & a_{22} & \cdots & 0      & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\     
      a_{i1} & a_{i2} & \cdots & a_{ii} & \cdots & 0 \\
      \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
      a_{n1} & a_{n2} & \cdots & a_{ni} & \cdots & a_{nn} 
    \end{pmatrix} 
    \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_j \\ \vdots \\ x_n \end{pmatrix} =
    \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_j \\ \vdots \\ b_n \end{pmatrix}
    \nonumber   
\end{equation} 
%
To solve this, solve the first equation for the one unknown, which becomes know. Then the second equation for the (now) one unknown, etc. This is called forward substitution.
%
\begin{align}
&x_1 = \frac{b_1}{a_{11}} \nonumber \\
&\text{for } i = 2, n \nonumber \\
&\qquad x_i = \frac{1}{a_{ii}}(b_i - \sum_{j=1}^{i-1} a_{ij} x_j) \nonumber
\end{align}

%--------------------------------------------------------------------
Upper triangular systems work the same way, but you go in the reverse order $\rightarrow$ backward substitution. 
%
\begin{align}
&x_n = \frac{b_n}{a_{nn}} \nonumber \\
&\text{for } i = n-1, -1, 1 \nonumber \\
&\qquad x_i = \frac{1}{a_{ii}}(b_i - \sum_{j=i+1}^{n} a_{ij} x_j) \nonumber
\end{align}

These both require $O(n^2)$ operations, and each have the same caveats about zeros as the diagonal system.

%--------------------------------------------------------------------
\subsection{LU Decomposition}

\textbf{Theorem}

Let $\ve{A} \in \mathcal{R}^{m \times n}$ be regular (recall: regular means $\ve{A}^{-1}$ exists). Then there are matrices $\ve{L}$ (lower triangular) with $l_{ii} = 1$ (not a requirement, but easier) and $\ve{U}$ (upper triangular) such that $\ve{A} = \ve{L}\ve{U}$ is unique.

We want to solve $\ve{A}\vec{x} = \vec{b}$:
%
\begin{align}
&\ve{A}\vec{x} = \ve{L}\ve{U}\vec{x} = \vec{b} \nonumber \\
%
&\text{define } \ve{U}\vec{x} = \vec{b}' \nonumber \\
%
&\text{then note }\ve{L}\vec{b}' = \vec{b} \nonumber
\end{align}
%
\begin{enumerate}
\item Solve $\ve{L}\vec{b}' = \vec{b}$ for $\vec{b}'$ using forward substitution
\item Solve $\ve{U}\vec{x} = \vec{b}'$ for $\vec{x}$ using backward substitution
\end{enumerate}
%
This requires $O(n^2)$ operations.

The \textbf{LU decomposition} is formed by Gaussian Elimination, requiring $O(n^3)$ operations. $\ve{U}$ is the upper triangular matrix from Gaussian Elimination; $\ve{L}$ is the lower triangular matrix (formed from the row multipliers). 
%
\begin{equation}
   \ve{L} = \begin{pmatrix}
      1      & 0      & \cdots    & 0 \\
      m_{21} & 1      & \cdots    & 0 \\
      \vdots & \vdots & \ddots    & \vdots \\     
      m_{n1} & \cdots & m_{n,n-1} & 1 
    \end{pmatrix} \qquad
  \ve{U} = \begin{pmatrix}
      a_{11} & a_{12} & \cdots & a_{1n} \\
      0      & a_{22} & \cdots & a_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\     
      0      & 0      & \cdots &  a_{nn} 
    \end{pmatrix}
    \nonumber   
\end{equation} 



\end{document}