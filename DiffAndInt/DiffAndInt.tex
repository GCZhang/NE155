%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Classes 13 \& 14, S14 \\
Differentiation \& Integration \\ February 24 and 26, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

%--------------------------------------------------------------------
\section{Differentiation}
\underline{Problem} Given a function $f(x)$ at $x_0, x_1, \dots, x_n$ approximate $f'(x)$ or $f''(x)$, etc. 

Numerical differentiation is  
\begin{itemize}
\item useful when a function is defined by 
\begin{itemize}
\item data
\item differential equations (ODEs, PDEs)
\end{itemize}
\item inherently sensitive to round-off error. 
\end{itemize}

\textbf{Example} Given $f \in C^2 [a,b]$ and let $x_0 \in [a,b]$, find an approximation to $f'(x_0)$.

We're going to come at this from \textbf{Taylor's theorem}.
%
\begin{align}
f(x) &= f(x_0) + f'(x_0)(x - x_0) + \frac{f''(c)}{2}(x-x_0)^2 \\
% 
\therefore f'(x_0) &= \frac{f(x) - f(x_0)}{(x - x_0)} - \frac{f''(c)}{2}(x-x_0) \\
%
\text{Let } h &= x-x_0 \rightarrow x = x_0 + h \\
%
\therefore f'(x_0) &= \underbrace{\frac{f(x_0 + h) - f(x_0)}{h}}_{\text{computable approx.}} - \frac{f''(c)}{2}h
\end{align}
%
We can extend this principle to get higher levels of accuracy. To do that we do a Taylor expansion for each point in our collection and choose how many points to combine and in what ways. 
%
\begin{center}
\begin{tikzpicture}
\draw (-.25,0)--(1.25,0);
\draw[dotted] (1.25,0)--(2.75,0);
\draw (2.75,0)--(5.25,0);
\draw[dotted] (5.25,0)--(6.75,0);
\draw (6.75,0)--(8.25,0);
%\draw (4,0)--(5.25,0);
\draw (0,-.25)--(0,.25);
\draw (1,-.25)--(1,.25);
%\draw (2,-.25)--(2,.25);
\draw (3,-.25)--(3,.25);
\draw (4,-.25)--(4,.25);
\draw (5,-.25)--(5,.25);
\draw (7,-.25)--(7,.25);
\draw (8,-.25)--(8,.25);
\node[below] at (0,-.25) {$x_0$};
\node[below] at (1,-.25) {$x_1$};
\node[below] at (3,-.25) {$x_{i-1}$};
\node[below] at (4,-.25) {$x_i$};
\node[below] at (5,-.25) {$x_{i+1}$};
\node[below] at (7,-.25) {$x_{n-1}$};
\node[below] at (8,-.25) {$x_n$};
\end{tikzpicture}
\end{center}

\begin{align}
f(x_0) &= f(x_0)\\
%
f(x_0 \pm h) &= f(x_0) \pm hf'(x_0) + h^2\frac{f''(x_0)}{2} \pm h^3\frac{f'''(x_0)}{6} + f^{(4)}(c_1)\frac{h^4}{24} \\
%
f(x_0 \pm 2h) &= f(x_0) \pm 2h f'(x_0) + 2 h^2 f''(x_0) \pm \frac{4}{3} h^3 f'''(x_0) + \frac{2}{3}h^4 f^{(4)}(c_2)
\end{align}


% ---------------------------------------------------------
\subsection{Forward difference:}
\underline{$O(h)$}: combine the point and the next point forward
\[f'(x_0) = \frac{f(x_0 + h) - f(x_0)}{h} - \frac{1}{2}hf''(c)\]

\underline{$O(h^2)$}: combine the point and the next two points forward
%
\begin{align}
a f(x_0) &+ b f(x_0 + h) + c f(x_0 + 2h) = f'(x_0) \\
%
a f(x_0) &+ b[f(x_0) + hf'(x_0) + h^2\frac{f''(x_0)}{2} + h^3\frac{f'''(x_0)}{6}] \nonumber \\
&+ c[f(x_0) + 2h f'(x_0) + 2 h^2 f''(x_0) + \frac{4}{3} h^3 f'''(x_0)] =f'(x_0) \\
%
(a + b &+ c)f(x_0) + h(b + 2c)f'(x_0) + h^2(\frac{b}{2} + 2c) f''(x_0) \nonumber \\
&+ h^3(\frac{b}{6} +  \frac{4c}{3}) f'''(\mu) = f'(x_0)
\end{align}
%
Use mean value theorem to get $\mu \in [c_1, c_2]$ to get only one term.

*KEY* set the coefficients to get what we want:
%
\begin{align}
(a + b + c) &= 0\\
h(b + 2c) &= 1\\
h^2(\frac{b}{2} + 2c) &= 0
\end{align}
%
We don't have more degrees of freedom, so we're left with $f'''$ for the error term. 

Solving all of that gives
%
\begin{align}
a &= -\frac{3}{2h} \\
b &= \frac{2}{h} \\
c &= \frac{1}{2h} \\
&\text{error term } = -\frac{1}{3}h^2 f'''(\mu) \\
\therefore \quad &\boxed{f'(x_0) = \frac{-3 f(x_0) + 4f(x_0 + h) - f(x_0 + 2h)}{2h} -\frac{1}{3}h^2 f'''(\mu)}
\end{align}


%--------------------------------------------------------------------
\subsection{Backward difference:}
Essentially, the signs all flip because we use points in the other direction.

\underline{$O(h)$}: combine the point and the next point backward
\[f'(x_0) = \frac{f(x_0) - f(x_0 - h)}{h} + \frac{1}{2}hf''(\mu)\]

\underline{$O(h^2)$}: combine the point and the next two points backward
%
\[f'(x_0) = \frac{3 f(x_0) - 4f(x_0 - h) + f(x_0 - 2h)}{2h} + \frac{1}{3}h^2 f'''(\mu)\]

%--------------------------------------------------------------------
\subsection{Central difference:}
Use points on either side (the average of forward and backward difference) $\rightarrow$ more accuracy for the same number of evaluation  points.

\underline{$O(h^2)$}: combine one point on either side
\[f'(x_0) = \frac{f(x_0 + h) - f(x_0 - h)}{2h} - \frac{1}{6}h^2 f'''(\mu)\]

\underline{$O(h^4)$}: combine two points on either side
\[f'(x_0) = \frac{f(x_0 - 2h) - 8f(x_0 - h) + 8f(x_0 + h) - f(x_0 + 2h)}{2h} - \frac{1}{30}h^4 f^{(5)}(\mu)\]

\subsubsection{Higher Order Derivatives}
By solving for different terms in the Taylor expansion, combining points, and solving for coefficients we can get other derivative terms as well. 

E.g. $a f(x_0 + h) + b f(x_0) + c f(x_0 - h) = f''(x_0)$

These are all $O(h^2)$:
\begin{align}
f''(x_0) &= \frac{f(x_0 - h) - 2f(x_0) + f(x_0 + h)}{h^2} + \frac{h^2}{12}f^{(4)}(\mu) \\
%
f^{(3)}(x_0) &= \frac{-f(x_0 - 2h) + 2f(x_0 - h) - 2f(x_0 + h) + f(x_0 + 2h)}{2h^3}\\
%
f^{(4)}(x_0) &= \frac{f(x_0 - 2h) - 4f(x_0 - h) +6f(x_0) - 4f(x_0 + h) + f(x_0 + 2h)}{h^4}
\end{align}

\subsubsection{Two Variables}
\begin{center}
\begin{tikzpicture}
\draw (-.25,0)--(7.25,0);
\draw (1,-.25)--(1,.25);
\draw (3,-.25)--(3,5.25);
\draw (4,-.25)--(4,5.25);
\draw (5,-.25)--(5,5.25);
\draw (7,-.25)--(7,.25);
\node[below] at (0,-.25) {$x_0$};
\node[below] at (1,-.25) {$x_1$};
\node[below] at (3,-.25) {$x_{i-1}$};
\node[below] at (4,-.25) {$x_i$};
\node[below] at (5,-.25) {$x_{i+1}$};
\node[below] at (7,-.25) {$x_n$};
\node at (3.5, 2) {$\Delta x$};
% begin y
\draw (0,-.25)--(0,7.25);
\draw (-.25,1)--(.25,1);
\draw (-.25,3)--(5.25,3);
\draw (-.25,4)--(5.25,4);
\draw (-.25,5)--(5.25,5);
\draw (-.25,7)--(.25,7);
\node[left] at (-.25, 0) {$y_0$};
\node[left] at (-.25,1) {$y_1$};
\node[left] at (-.25,3) {$y_{i-1}$};
\node[left] at (-.25,4) {$y_i$};
\node[left] at (-.25,5) {$y_{i+1}$};
\node[left] at (-.25,7) {$y_n$};
\node at (2,3.5) {$\Delta y$};
\end{tikzpicture}
\end{center}

\begin{align}
\frac{\partial^2 f_{i,j}}{\partial x^2} = \frac{\partial}{\partial x}\bigl(\frac{\partial f_{i,j}}{\partial x}\bigr) =
\frac{f_{i-1,j} - 2f_{i,j} + f_{i+1,j}}{\Delta x^2} \\
%
\frac{\partial^2 f_{i,j}}{\partial y^2} = \frac{\partial}{\partial y}\bigl(\frac{\partial f_{i,j}}{\partial y}\bigr) =
\frac{f_{i,j-1} - 2f_{i,j} + f_{i,j+1}}{\Delta y^2}
\end{align}



%--------------------------------------------------------------------
%-------------------------------------------------------------------
\section{Integration}
Why might we need to numerically integrate?
\begin{itemize}
\item The integrand, $f(x)$, is complicated enough so that its
indefinite or even definite integral over a special range cannot
be found.
\item The indefinite or definite integral can be found, but may take
long time to calculate it analytically.
\item The integrand is given in terms of a table. That is, we are
provided with discrete values of $f(x)$ at $n+1$ data or base
points.
\end{itemize}

The general strategy is:
\begin{itemize}
\item Approximate the integrand $f(x)$ with either a global interpolating
polynomial defined over the whole domain of integration, or a
collection of local interpolating polynomials defined over intervals
that are subtended by a small group of data points.
\item Integrate the interpolating polynomials over their individual
domains of definition.
\end{itemize}

Thus, given $f \in C[a,b]$ compute $I(f) = \int_a^b f(x) dx$.

The way we do this is called numerical integration or numerical quadrature:
\[I(f) \approx I_n(f) = \sum_{i=0}^n a_i f(x_i)\]
$a_i \equiv$ quadrature weights\\
$x_i \equiv$ quadrature points

\textbf{Approach \#1}\\
Fix quadrature points ($x_i$), then choose quadrature weights ($a_i$). Usually fit a polynomial to $f(x_i)$ and integrate.
%
\begin{enumerate}%[a.]
\item \underline{Newton-Cotes}: use a single poly over the entire interval
\item \underline{composite Newton-Cotes}: split the interval
\item \underline{Romberg integration}: extrapolation
\end{enumerate}

\textbf{Approach \#2}\\
For a given $n$, choose the ``best" quadrature points ($x_i$) and quadrature weights ($a_i$):\\
\underline{Gaussian Quadrature}

the \textbf{degree of precision} of a quadrature formula, $I_n(f)$, is the positive integer $m$ s.t.:
\begin{enumerate}
\item $I(p) = I_n(p)$ for every poly of degree $\leq m$.
\item $I(p) \neq I_n(p)$ for some poly of degree $m+1$.
\end{enumerate}
 
 
%--------------------------------------------------------------------
\subsection{Newton-Cotes formula}
 
\underline{idea}: fix $x_0, x_1, \dots, x_n$ then interpolate $f(x)$ by a poly of degree $\leq n$.
%
\begin{align}
I(f) &\approx I_n(f) = I(p_n)\\
\text{true integral } &\approx \text{ N-C formula } = \text{ true integral of interpolating poly}
\end{align}
%
The only error is in the interpolation.

%--------------------------------------------------------------------
\subsection*{Lagrange form}
Choosing Lagrange polynomials prevents us from needing to recalculate weights for every $f(x)$ (since we've fixed the points).

\begin{align}
P_n(x) &= \sum_{i=0}^{n}f(x_i)L_i(x) \\
%
I(P_n(x)) &= \sum_{i=0}^{n} \bigl[ \int_a^b L_i(x)dx \bigr] f(x_i) \\
%
&= \sum_{i=0}^{n} a_i f(x_i)\\
\text{where } a_i &=  \int_a^b L_i(x)dx \qquad \therefore a_i \text{ are not dependent on } f(x)
\end{align}

%--------------------------------------------------------------------
\subsubsection{Trapezoid Rule}
This is the Lagrange form of Newton-Cotes integration with $n=1$.

$x_0 = a$, $x_1 = b$, $h = x_1 - x_0$
%
\begin{align}
&L_0(x) = \frac{x-x_1}{x_0-x_1} \qquad L_1(x) = \frac{x-x_0}{x_1-x_0} \\
%
a_0 &= \int_{x_0}^{x_1} \bigl(\frac{x-x_1}{x_0-x_1}\bigr) dx \qquad \text{let } x = x_0 + th \:; dx = hdt \\
%
a_0 &= h \int_0^1 \bigl(\frac{x_1 - x_0 - ht}{h}dt = \int_0^1 h(1-t)dt \\
%
a_0 &= h\bigl(t - \frac{1}{2}t^2 \bigr) |_0^1 = \frac{1}{2}h\\
%
a_1 &= \int_{x_0}^{x_1} \bigl(\frac{x-x_0}{x_1-x_0}\bigr) dx = \frac{1}{2}h\\
%
& \therefore I_1(f) = \frac{h}{2}\bigl(f(x_0) + f(x_1)\bigr) \rightarrow \text{area of a trapezoid}
\end{align}

The trapezoid rule has degree of precision 1.

\textit{DRAW PICTURE}

%--------------------------------------------------------------------
\subsubsection{Error Analysis}
\begin{enumerate}
\item interpolate:
\[f(x) = P_n(x) + R_n(x)\]
\item integrate:
\[\int_a^b f(x) dx = \int_a^b P_n(x) dx + \int_a^b R_n(x) dx\]
\end{enumerate}

\textbf{Example:} Trapezoid Rule\\
\begin{align}
R_1(x) &= \frac{f''(c)}{2!}(x-x_0)(x-x_1) \\
I(f) - I_1(f) &= \int_{x_0}^{x_1} \frac{f''(c)}{2!}\underbrace{(x-x_0)(x-x_1)}_{\text{doesn't change sign over interval}} dx
\end{align}
%
Recall that $c$ depends on $x$. To get an expression, we're going to use the mean value theorem (which I briefly referenced last time).

\underline{Mean Value Theorem for Integrals}\\
\[\int_a^b f(x)g(x)dx = f(\xi) \int_a^b g(x)dx \qquad a\leq \xi \leq b\]
$g(x)$ must always be positive or always negative (cannot change sign over the interval).

Thus, we can use MVT for our integral to see that
\begin{align}
I(f) - I_1(f) &= \frac{1}{2}f''(\eta) \int_{x_0}^{x_1} (x-x_0)(x-x_1) dx \\
&= \frac{1}{2}f''(\eta) \bigl(\frac{-h^3}{6}\bigr) \rightarrow \boxed{\frac{-h^3}{12}f''(\eta)}
\end{align}
is the error term for trapezoid rule. You can see that it's first order b/c the second derivative drives the error.


%--------------------------------------------------------------------
\subsubsection{Simpson's Rule}
This is the Lagrange form of Newton-Cotes integration with $n=2$.

\begin{align}
x_0 &= a \qquad x_1 = \frac{a+b}{2} \qquad x_2=b \qquad h=\frac{b-a}{2}\\
%
L_0 &= \frac{(x-x_1)(x-x_2)}{(x_0-x_1)(x_0-x_2)}\\
L_1 &= \frac{(x-x_0)(x-x_2)}{(x_1-x_0)(x_1-x_2)}\\
L_2 &= \frac{(x-x_0)(x-x_1)}{(x_2-x_0)(x_2-x_1)}\\
%
a_0 &= \int_{a=x_0}^{b=x_2} L_0(x)dx = \frac{h}{3}\\
a_1 &= \int_{a=x_0}^{b=x_2} L_1(x)dx = \frac{4h}{3}\\
a_2 &= \int_{a=x_0}^{b=x_2} L_2(x)dx = \frac{h}{3}\\
&\text{Note: $a$s sum to }2h\text{, the size of the interval}\nonumber\\
%
&\therefore I_2(f) = \frac{h}{3}\bigl(f(x_0) + 4f(x_1) + f(x_2)\bigr) \end{align}
 
The error term comes from integrating $R_2 = \frac{1}{3!}f'''(c)(x-x_0)(x-x_1)(x-x_2)$. However, the middle term now changes sign over the interval and we can't just use MVT.

We're going to skip the details, but what we get out is that the error for Simpson's Rule looks like:
\[\frac{-f^{(4)}(\eta)}{4!}\frac{h^5}{\frac{4}{15}} \rightarrow \boxed{\frac{-f^{(4)}(\eta)}{90}h^5}\]

Thus, Simpson's rule has degree of precision 3 and is $O(h^5)$.

\vspace*{2em}
\textbf{In General:}
\begin{center}
\begin{tabular}{c c c}
$n$  & error        & precision \\ \hline
odd  & $O(h^{n+2})$ & $n$ \\
even & $O(h^{n+3})$ & $n+1$ \\
\end{tabular}
\end{center}

\underline{Simpson's 3/8 rule}:\\
If $n=3$:
\[\int_{x_0}^{x_3} f(x)dx = \frac{3h}{8}\bigl(f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)\bigr) - \frac{3h^5}{80}f^{(4)}(\xi)\]
where $x_0 < \xi < x_3$ and $h=(x_3 - x_0)/3$.

\subsubsection{Composite Newton-Cotes}
Some difficulties:
\begin{enumerate}
\item To add points we must recompute the weights, $a_i$, which can be a lot of work.
\item high order polynomial interpolation on equally spaced points is BAD. (using optimally-spaced points is Gaussian quadrature; we'll get to that).
\end{enumerate}

\underline{Idea:} subdivide $[a,b]$ into subintervals. On each of these apply a low-order Newton-cotes formula.

\textbf{Composite Trapezoid}:\\
Just apply the trapezoid rule on each subinterval. Given $x_0, x_1, \dots, x_n$ and $f(x_0), f(x_1), \dots f(x_n)$, the grid space is $h = \frac{x_n-x_0}{n}$.

\textit{DRAW PICTURE}

\begin{align}
\int_{x_0}^{x_n} f(x)dx &= \sum_{i=1}^n \int_{x_{i-1}}^{x_i} f(x)dx\\
&= \sum_{i=1}^n \bigl[ \frac{h}{2}\bigl(f(x_{i-1}) + f(x_i)\bigr) - \frac{h^3}{12}f''(c_i) \bigr] \\
&= \frac{h}{2}\bigl(f(x_0) + 2\sum_{i=1}^{n-1}f(x_i) + f(x_n)\bigr) - \frac{h^3}{12} \underbrace{\sum_{i=1}^n f''(c_i)}_{n f''(\mu), n = (x_n-x_0)/h}\\
%
\int_{a}^{b} f(x)dx &= \boxed{\frac{h}{2}\bigl(f(x_0) + 2\sum_{i=1}^{n-1}f(x_i) + f(x_n)\bigr) - \frac{h^2}{12}(b-a)f''(\mu) }
\end{align}
 
\textbf{Composite Simpson}:\\
Similarly, apply the Simpson rule on each subinterval. %Given $x_0, x_1, \dots, x_n$ and $f(x_0), f(x_1), \dots f(x_n)$, the grid space is $h = \frac{x_n-x_0}{n}$. 
%
\begin{align}
\int_{x_0}^{x_n} f(x)dx &= \sum_{i=2}^n \bigl[ \frac{h}{3}\bigl(f(x_{i-2}) + 4f(x_{i-1}) + f(x_i)\bigr) - \frac{f^{(4)}(c_i)}{90}h^5 \bigr]\\
&= \frac{h}{3}\bigl(f(x_0) + 4\sum_{\substack{i=1\\i=\text{odd}}}^{n-1} f(x_i) + 2\sum_{\substack{i=2\\i=\text{even}}}^{n-1} f(x_i) + f(x_n)\bigr) - \frac{h^4}{90}(b-a)f^{(4)}(\mu)
\end{align}


%--------------------------------------------------------------------
\subsubsection{Open Newton-Cotes}
The previous examples were called \textbf{closed} Newton-Cotes because $f(x)$ is evaluated at the first and last points.

In \textbf{open} Newton-Cotes does not evaluate $f(x)$ at the endpoints. The node points are defined as $x_{j} = x_0 + jh, j = 0, \dots, n$, $h = (b-a)/(n+2)$. However, we define $x_{0} = a + h$, which implies $x_{n} = b - h$. Now our real endpoints are $x_{-1}$ and $x_{n+1}$ (so we're doing $\int_{x_{-1}}^{x_{n+1}} f(x)dx$).


\begin{figure}
\begin{center}
  \includegraphics[height=2in,clip]{ClosedNewtonCotes}
\end{center}
\end{figure}
\begin{figure}
\begin{center}
  \includegraphics[height=1.75in,clip]{OpenNewtonCotes}
\end{center}
\end{figure}


The \underline{midpoint rule} is what we get with \underline{$n=0$}: it only uses one point:
\[\int_a^b f(x)dx = \int_{x_{-1}}^{x_{1}} f(x)dx = 2hf(x_0) + \frac{h^3}{3}f''(\xi)\]
where $x_{-1} < \xi < x_{1}$ and $h=(b-a)/2$.

\underline{$n=1$}:
\[ \int_{x_{-1}}^{x_{2}} f(x)dx = \frac{3h}{2}[f(x_0) + f(x_1)] + \frac{3h^3}{4}f''(\xi)\]
where $x_{-1} < \xi < x_{2}$ and $h=(b-a)/3$.

\underline{$n=2$}:
\[ \int_{x_{-1}}^{x_{3}} f(x)dx = \frac{4h}{3}[2f(x_0) - f(x_1) + 2f(x_2)] + \frac{14h^5}{45}f^{(4)}(\xi)\]
where $x_{-1} < \xi < x_{3}$ and $h=(b-a)/4$.

\begin{figure}
\begin{center}
  \includegraphics[height=3.5in,clip]{QuadratureComparison}
\end{center}
\end{figure}

%--------------------------------------------------------------------
%--------------------------------------------------------------------
%\bibliographystyle{plain}
%\bibliography{LinearSolns} 

\end{document}