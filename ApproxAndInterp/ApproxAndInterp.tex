%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)
% Spring 2014

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Classes 11 \& 12, S14 \\
Approximation and Interpolation \\ February 14 and 19, 2014}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

%--------------------------------------------------------------------
\section{Introduction}
Sometimes rather than calculating values of a function, we need to approximate or interpolate them instead. Why might we do this?
%
\begin{itemize}
\item It may be difficult or impossible to analytically evaluate the function
\item We may have only a table of values at certain points and need to
determine the values in between
\item It may be much faster to compute values of an approximation
function than of the original function, particularly if we have to
calculate the function values over and over again
\item A function may be defined implicitly
\end{itemize}

This is relevant in numerical \textbf{integration/differentiation} and in \textbf{solving differential equations}.

\underline{Problem:} given a function $f(x)$ or a set  of discrete values $\{x_i, f(x_i)\}$, try to find $g(x)$ that in some sense approximates $f(x)$. 

\begin{enumerate}
\item \textbf{Approximation} The approximation function does \underline{not} need to fit through all given function values. 

Why would we do this? Perhaps we know there are errors in the data, or we only care about the general trend. 

Example: Least Squares

\item \textbf{Interpolation} The interpolation function does need to do
through all give values of the function.

We do this because we believe the data points. We will discuss this strategy first.
\end{enumerate}

%--------------------------------------------------------------------
\subsection{Types}
There are several common types of Interpolating functions
%
\begin{itemize}
\item \textbf{Polynomial}: $g(x) = P(x) = \sum_i a_i x^i$

\item \textbf{Piecewise polynomial}: slap some polynomials together so their ends meet

\item \textbf{Trig functions}: e.g., Fourier series $g(x) = \frac{1}{2} a_0 + \sum_k [a_k \sin(kx) + b_k \cos(kx)]$ %http://mathworld.wolfram.com/FourierSeries.html

\item Combination of \textbf{rational functions}: $g(x) = R_n(x)/R_m(x)$

\item \textbf{Exponential functions}: $g(x) = \sum_i a_i \exp(b_i x^i)$
\end{itemize}


%--------------------------------------------------------------------
\section{Polynomial Interpolation}
Why polynomials?
\[g(x) = a_0 + a_1 x + a_2 x^2 + \dots +a_n x^n\]

\begin{enumerate}
\item derivatives and integrals are easy to compute $\rightarrow$ after either operation we still have a polynomial
\item efficient to evaluate
\[g(x) = a_0 + x(a_1 + x(a_2 + x(\dots)))\]
\end{enumerate}

\underline{Theorem:} ``Weierstrauss approx theorem"

let $f$ be $C^0 \in [a,b]$. Give $\epsilon > 0$, $\exists$ a polynomial, $P(x)$, such that 
\[|f(x) - p(x)| \leq \epsilon, x \in [a,b] \:.\]

%--------------------------------------------------------------------
%--------------------------------------------------------------------
\subsection{Taylor Polynomial}
If $f(x)$ is known but difficult to evaluate, we can replace it by the
Taylor polynomial around x = a
\begin{align}
f(x) &= f(a) + f'(a)(a-x) + f''(a)\frac{(x-a)^2}{2!} + \cdots + f^{(n-1)}(a)\frac{(x-a)^{n-1}}{(n-1)!} + R_n \\
%
  &= \sum_{k=0}^{n-1}\frac{(x-a)^k}{k!}f^{(k)}(a) + R_n \\
%
R_n &= f^{(n)}(x^*)\frac{(x-a)^{n}}{n!} \qquad \text{for some } x^* \in [a, x]
%http://mathworld.wolfram.com/TaylorSeries.html
\end{align}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\subsection{Lagrange Interpolating Polynomial}
%
\underline{Problem:} Given $x_0, x_1,\dots, x_n$ are $(n+1)$ distinct points find $P_n(x)$, a unique polynomial of degree $\leq n$ s.t.\
\[f(x_k) = P(x_k)\:, \qquad \text{for }k= 0, 1, \dots, n\]
%
UPDATE THE REST OF THIS SECTION FROM R NOTES!

\begin{align}
P(x) &= f(x_0)L_0(x) + \dots + f(x_n)L_n(x) = \sum_{k=0}^{n}f(x_k)L_k(x) \\
%
L_k(x) &= \prod_{i=0, i \neq k}^n \frac{(x-x_i)}{(x_k-x_i)}\\
%
L_k(x) &= \frac{(x-x_0)(x-x_1)\cdots(x-x_{k-1})(x-x_{k+1})\cdots(x-x_n)}{(x_k-x_0)(x_k-x_1)\cdots(x_k-x_{k-1})(x_k-x_{k+1})\cdots(x_k-x_n)}
\end{align}

Note: this is sometimes called full degree polynomial interpolation.

\textbf{iPython demo; show Taylor picture as well.}

%--------------------------------------------------------------------
\subsubsection{Error}
\underline{Theorem:} If $x_0, x_1,\dots, x_n$ are $(n+1)$ distinct points and $f$ is defined at those points, then $\exists$ a unique polynomial, $P_{n}(x)$ of degree $ \leq n$, that interpolates $f$ at the $n + 1$ distinct points: $f(x_k) = P(x_k)$

\underline{Theorem:} If $x_0, x_1,\dots, x_n$ are $(n+1)$ distinct points $\in [a,b]$ and $f$ is $C^{n+1} \in [a,b]$ then for each $x\in [a,b]$ there exists $\xi(x) \in [a,b]$ s.t.
\begin{align}
f(x) - P_n(x)& = \frac{f^{n+1}(\xi)}{(n+1)!}(x-x_0)(x-x_1)\cdots(x-x_n) \\
&\boxed{e = \frac{f^{n+1}(\xi)}{(n+1)!}\prod_{i=0}^n (x-x_i)}
\end{align}
%
%The Lagrange polynomial of degree $n$ uses information at the
%distinct numbers $x_0, x_1,\dots, x_n$ and, in place of $(x-x_0)^n$, its error formula uses a product of the $n+1$ terms.
%
%For all polynomial interpolations, the error is related to the $n+1$st derivative of the function. If we have more points than derivatives, this might not work well. This is related to our next idea.

What is a variable here that will affect the error? How we choose the $x_i$. We will not get into this here, but in general equally spaced points don't work terribly well and there are sets of differently spaced points that are designed to be optimal in different ways. 

%--------------------------------------------------------------------
%--------------------------------------------------------------------
\subsection{Piecewise Polynomials}

It is often difficult to get one polynomial to fit all of our data very well, and we'd rather not have to select optimal points (since it will depend on the norm we use). 

An alternative approach is to divide the interval into small sub-intervals (usually of equal size) and construct a polynomial on each-subinterval. This is called ``splines".

%If you think about our error analysis, we'd like (these two competing things)
%\begin{enumerate}
%\item to be able to differentiate our function to a degree that allows us to do error analysis given our polynomial. We don't know how many times our function is differentiable, but if we shoot low it's more likely to work. 
%
%Thus, we use lower order polynomials all stuck together.
%
%Note: the construction of the polynomials does not assume that the derivatives of the interpolant agree with the derivative of the function.
%
%\item our polynomials to be as continuously differentiable at the boundaries as possible.
%
%Thus, we often select splines. 
%\end{enumerate}
%
A \textbf{spline} is a polynomial function that is piecewise-defined, and possesses a ``high" degree of smoothness at the places where the polynomial pieces connect (which are known as \textit{knots}).


The simplest way to do this is \textbf{piecewise linear}. What might be a problem with that? Piecewise linear is only $C^0$.

The most common approach is to use \textbf{cubic splines}: a cubic polynomial between each successive pair of nodes. For cubic splines, we can get continuous 2nd derivatives at the knots - which is all we can ask from cubic polynomials. 

Given a function $f(x)$ defined on $[a,b]$ and a set of numbers (knots) $a=x_0 < x_1 < ... < x_n = b$, a cubic spline interpolant, $S(x)$, for $f$ is:
\begin{itemize}
\item on each sub-interval $[x_j, x_{j+1}]$, $S$ is a cubic polynomial:
\[S(x) = S_j(x) = a_j + b_j(x-x_j) + c_j(x-x_j)^2 + d_j(x-x_j)^3 \text{ for } j = 0, 1, ..., n-1\]

\item $S$ interpolates $f$ (n+1 constraints)
\[S(x_j) = f(x_j)\text{ for } j = 0, 1, ..., n-1\]

\item $S$ is continuous (n-1 constraints)
\[S_{j+1}(x_{j+1}) = S_{j}(x_{j+1})\text{ for } j = 0, 1, ..., n-2\]

\item $S'$ is continuous (n-1 constraints)
\[S'_{j+1}(x_{j+1}) = S'_{j}(x_{j+1})\text{ for } j = 0, 1, ..., n-2\]

\item $S''$ is continuous (n-1 constraints)
\[S''_{j+1}(x_{j+1}) = S''_{j}(x_{j+1})\text{ for } j = 0, 1, ..., n-2\]
\end{itemize}
%
With $n$ subintervals we have $4n$ unknowns and only $4n-2$ constraints!

Thus, we select one of these boundary conditions:
\begin{itemize}
\item ``not-a-knot": remove two analysis points
\item ``free" or ``natural": $S''(x_0) = S''(x_n) = 0$ 
\item ``clamped" or ``complete": $S'(x_0) = f'(x_0)$ and $S'(x_n) = f'(x_n)$ 
\end{itemize}

Which of these do you expect to be the most accurate and why? The least?
\begin{itemize}
\item ``not-a-knot": least accurate because you're removing data
\item ``free" or ``natural": introduces $O(h^2)$ error near the end points.
\item ``clamped" or ``complete": most accurate, but we need to know the end point derivatives.
\[e = \max_{a \leq x \leq b} |f(x) - S(x)| \leq \frac{5}{384}h^4 M\]
where
\[M = \max_{a \leq x \leq b} |f(x)^{(4)}|\]
and
\[h = \max_{0 \leq j \leq n-1} (x_{j+1} - x_{j})\]
\end{itemize}

We're not going to go over the spline algorithm, but you get a tridiagonal system out of it.

The Python function for this is \textit{scipy.interpolate.splrep}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\section{Approximation}

What if we have some collection of data, $(x_0, y_0), (x_1, y_1),\dots, (x_m, y_m)$ that is noisy but obeys, what we believe, is some functional form (linear, exp., trig., etc.)?

We can approximate the data to get an expression for the functional form.

%--------------------------------------------------------------------
\subsection{Least Squares}
Least Squares finds the best approximating line that minimizes the least squares error of the representation. 

\subsubsection{Linear}
If we assume the form is linear, we are trying to find
\[y = a_0 + a_1 x\]
where these coefficients provide the best fit for
 
\begin{align}
y_0 &= a_0 + a_1 x_0 \\
y_1 &= a_0 + a_1 x_1 \\
& \vdots \\
y_m &= a_0 + a_1 x_m \\
%
\begin{pmatrix}
1 & x_0 \\ 1 & x_1 \\ \vdots & \vdots \\ 1 & x_m
\end{pmatrix}
\underbrace{\begin{pmatrix}
a_0 \\ a_1
\end{pmatrix}}_{\vec{z}} &=
\begin{pmatrix}
y_0 \\ y_1 \\ \vdots \\ y_m
\end{pmatrix}
\end{align}
%
Why can't we just solve this matrix system? $n < m$ and the system is overdetermined. Instead, we look to minimize the error, $e = ||\ve{A}\vec{z} - \vec{b}||$.

We choose the 2-norm. The 2-norm squared is
\[e = \sum_{i=0}^m \bigl(y_i - (a_o + a_1 x_i)\bigr)^2\]
%
To minimize this wrt the $a$s, we take the partial derivative and set equal to zero:
%
\begin{align}
\frac{\partial e}{\partial a_0} &= -2 \sum_{i=0}^m \bigl(y_i - (a_0 + a_1 x_i)\bigr) = 0 \\
%
\frac{\partial e}{\partial a_1} &= -2 \sum_{i=0}^m x_i \bigl(y_i - (a_0 + a_1 x_i)\bigr) = 0 \\
\end{align}
%
This gives
%
\begin{equation}
\begin{pmatrix}
m                & \sum_{i=0}^m x_i \\
\sum_{i=0}^m x_i & \sum_{i=0}^m x_i^2 \\ 
\end{pmatrix}
\begin{pmatrix}
a_0 \\ a_1
\end{pmatrix} =
\begin{pmatrix}
\sum_{i=0}^m y_i \\ \sum_{i=0}^m x_i y_i
\end{pmatrix}
\end{equation}
%
and now we have two equations with two unknowns! 

\underline{note:} If you look at the \ve{A} we had originally defined, \ve{A}$^T$\ve{A} and \ve{A}$^T\vec{b}$ also gives this system. These are called the normal equations
%
\begin{align}
\ve{A}^T\ve{A} \vec{z} &= \ve{A}^T\vec{b} \\
%
\det(\ve{A}^T\ve{A}) &= m \sum_{i=0}^m x_i^2 -  \bigl(\sum_{i=0}^m x_i\bigr)^2 \\
%
&= \frac{1}{2} \sum_{i=0}^m \sum_{j=0}^m (x_i - x_j)^2
\end{align}
%
The determinant is 0 iff $x_i=c \forall i$.

%--------------------------------------------------------------------
\subsubsection{Polynomial}
\[y = a_0 + a_1 x + a_2 x^2 + \dots +a_n x^n\]
If you have $> n+1$ constraints, use least squares by constructing the matrix where $m>n$ and solving the normal equations.

\textbf{draw matrix}

%--------------------------------------------------------------------
\subsubsection{Exponential}
If the functional form is
%
\begin{align}
y &= \beta e^{\alpha x} \text{ or}\\
y &= \beta x^{\alpha}
\end{align}
%
You can do the least square procedure with taking the two-norm and taking the partial derivative

-or- 

you can simply take the log and treat it like the linear case
%
\begin{align}
\underbrace{ln(y)}_{y} &= \underbrace{ln(\beta)}_{a_0} + \underbrace{\alpha}_{a_1} x \text{ or}\\
\underbrace{ln(y)}_{y} &= \underbrace{ln(\beta)}_{a_0} + \underbrace{\alpha}_{a_1} \underbrace{ln(x)}_{x}
\end{align}

%%--------------------------------------------------------------------
%\subsubsection{Fourier Series}
%Least Squares and Fourier Series are actually related to one another. 
%
%It is possible to approximate a function with a polynomial, as we know, but now we will write the way we're considering the error a bit differently:
%%
%\begin{align}
%e &=\int_a^b \bigl[f(x) - P_n(x)\bigr]^2 dx \\
%\text{where }& P_n(x) = \sum_{j=0}^{n} c_i \phi_j(x)
%\end{align}
%
%In general (often?), approximations to functions require that the basis set be orthogonal. 
%
%A \textbf{basis} is a linearly independent set of vectors. Given a basis of a vector space, every element of the vector space can be expressed uniquely as a finite linear combination of basis vectors.
%
%$\{ \phi_0, \phi_1, \dots, \phi_n\}$ is an orthogonal set of functions over $[a,b]$ w.r.t. a weight function, $w(x)$, if
%%
%\begin{equation}
%\int_a^b w(x) \phi_j(x) \phi_k(x) dx = \biggl\{
%\begin{array}{ll}
%0 & \mbox{if } j \neq k \\
%\alpha_x & \mbox{if } j = k
%\end{array}
%\end{equation}
%%
%if $\alpha = 1$, then the functions are also orthonormal.



%--------------------------------------------------------------------
%--------------------------------------------------------------------
%\bibliographystyle{plain}
%\bibliography{LinearSolns} 

\end{document}
