%--------------------------------------------------------------------
% NE 155 (intro to numerical simulation of radiation transport)

% formatting
\documentclass[12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{setspace}
\onehalfspacing

\setlength{\parindent}{0mm} \setlength{\parskip}{1em}


% packages
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}
\usepackage{amsmath}
\usepackage{graphicx} % for graphics files

% Draw figures yourself
\usepackage{tikz} 

% The float package HAS to load before hyperref
\usepackage{float} % for psuedocode formatting
\usepackage{xspace}

% from Denovo methods manual
\usepackage{mathrsfs}
\usepackage[mathcal]{euscript}
\usepackage{color}
\usepackage{array}

\usepackage[pdftex]{hyperref}

\newcommand{\nth}{n\ensuremath{^{\text{th}}} }
\newcommand{\ve}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\macro}{\ensuremath{\Sigma}}
\newcommand{\vOmega}{\ensuremath{\hat{\Omega}}}

\newcommand{\cc}[1]{\ensuremath{\overline{#1}}}
\newcommand{\ccm}[1]{\ensuremath{\overline{\mathbf{#1}}}}


%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
\begin{center}
{\bf NE 155, Class X, S16 \\
 Eigenvalues DATE, 2016}
\end{center}

\setlength{\unitlength}{1in}
\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

\section*{Eigenvalue Review}

Eigenvalues are a special set of scalars associated with a linear system of equations (a matrix equation) that are sometimes also known as characteristic roots. 

Each eigenvalue is paired with a corresponding so-called eigenvector (or, in general, a corresponding right eigenvector and a corresponding left eigenvector; there is no analogous distinction between left and right for eigenvalues).

The decomposition of a square matrix $\ve{A}$ into eigenvalues and eigenvectors is known as eigen decomposition, and the fact that \textbf{this decomposition is always possible as long as the matrix consisting of the eigenvectors of $\ve{A}$ is square} is known as the eigen decomposition theorem.

Let $\ve{A} \in \mathbb{R}^{n \times n}$. If there is a vector $\vec{x} \in \mathbb{R}^{n}$ such that
%
\[\ve{A} \vec{x} = \lambda \vec{x}\]
%
for some scalar $\lambda$, then $\lambda$ is an eigenvalue of $\ve{A}$ with a corresponding (right) eigenvector $\vec{x}$. Note that the eigenvalue represents the ``stretching factor" in the direction of its associated eigenvector. 

(http://math.stackexchange.com/questions/54176/is-there-a-geometric-meaning-of-the-frobenius-norm)
 
We find eigenvalues by re-writing the stated relationship as $(\ve{A} - \lambda \ve{I})\vec{x}=0$.

A linear system of equations has nontrivial solutions iff the determinant vanishes (Cramer's rule), so the solutions of this equation are given by
%
\[\det(\ve{A} - \lambda \ve{I})=0 \:.\] 	
%
This equation is known as the characteristic equation of $\ve{A}$, and the left-hand side is known as the characteristic polynomial.

\textbf{Example}
\begin{align}
    \ve{A} &= \begin{pmatrix}
        2 & 3 \\
        3 & -6
    \end{pmatrix} 
    &\ve{A} - \lambda \ve{I} = \text{det}\begin{pmatrix}
        2 - \lambda & 3 \\
        3 & -6 - \lambda
    \end{pmatrix} \nonumber \\
%  
\det(\ve{A} - \lambda \vec{I}) &= (2 - \lambda)(-6 - \lambda) - (3)(3) \nonumber \\
%
&= -12 + 6\lambda -2\lambda + \lambda^2 -9 \nonumber \\
&= \lambda^2 + 4\lambda -21 = 0 \nonumber \\
0 &= (\lambda - 3)(\lambda + 7) \nonumber \\
&\boxed{\lambda = 3, -7} \nonumber
\end{align} 


\begin{itemize}
\item Eigenvalues may be real or complex. 

\item Since an \nth degree polynomial has $n$ roots, $\ve{A}_{n \times n}$ is guaranteed to have $n$ real and/or complex eigenvalues, some of which may be repeated: $\lambda_1, \lambda_2, \dots, \lambda_n$.

This gives $\ve{A}\vec{u}_1 = \lambda_1 \vec{u}_1$, etc.

\item Left eigenvectors are found by reformulating the equation as $\vec{y}\ve{A} = \alpha \vec{y}$. In nuclear, we're used to seeing the right eigenvector formulation.
\end{itemize}


The spectrum of eigenvalues of a matrix $\ve{A}$ are defined formally as
\[\sigma(\ve{A}) = [ \lambda \in \mathbb{C} : \det(\ve{A} - \lambda \ve{I})=0] \] 
and an eigenvalue is 
\[ \lambda \in \sigma(\ve{A}) \]
%
\begin{itemize}
\item $\sigma(\ve{A}) = \sigma(\ve{A}^T)$
\item $\cc{\sigma(\ve{A})} = \sigma(\ve{A}^H)$
\end{itemize}

%--------------------------------------------------------------------------
\subsection*{Aside about Singular Values}

\underline{Note:} Eigenvalues/vectors are only for square matrices. The analog for rectangular matrices is singular values. We are not going to focus on these, since we will be dealing with square matrices. However, here's a quick aside:

If a square matrix $\ve{A}$ has linearly independent eigenvectors, it can be factored as $\ve{A} = \ve{P}\ve{D}\ve{P}^{-1}$. Here $\ve{D}$ is a diagonal matrix containing the eigenvalues of $\ve{A}$ and $\ve{P}$ contains the corresponding, linearly independent eigenvectors of $\ve{A}$.  

You cannot do this for a rectangular matrix. However, \textbf{any} matrix $\ve{A} \in \mathbb{C}^{m \times n}$ can be factored as $\ve{Q}\ve{\Sigma}\ve{V}^{-1}$, and this is called the singular value decomposition. $\ve{Q}$ and $\ve{V}$ are unitary, and $\ve{\Sigma}$ is diagonal where the entries are the singular values of $\ve{A}$: $\sigma_1, \sigma_2, \dots, \sigma_n$ with $\sigma_1 \geq \cdots \geq \sigma_n$. 

This comes from the notion that $\ve{A}^H\ve{A}$ is symmetric and can be orthogonally diagonalized (get linearly-independent eigenvectors). We can also say it as the singular values of $\ve{A}$ are the square roots of the eigenvalues of $\ve{A}^H\ve{A}$. 

To see the geometric meaning we can note that 
\[\ve{A} = \ve{Q} \ve{\Sigma} \ve{V}^{-1} \rightarrow \ve{A}\ve{V} = \ve{Q} \ve{\Sigma} \:,\]
look at one column at a time and see
\[\ve{A}\vec{v}_i = \sigma_i \vec{v}_i \:, \qquad i = 1, \cdots, n \:.\]

Now you can see that these are like eigenvalues in terms geometric stretching. And finally we note that $||\ve{A}||_F = \sqrt{\sigma_1^2 + \sigma_2^2 + \dots + \sigma_n^2}$, effectively meaning that the Frobenius norm measures the size of the distortion a matrix creates in a system. (see this reference for a good explanation: \href{http://www.math.iit.edu/~fass/477577_Chapter_2.pdf}{http://www.math.iit.edu/$\sim$fass/477577\_Chapter\_2.pdf}).


%--------------------------------------------------------------------------
\subsection*{Spectral radius} 

The spectral radius of $\ve{A} \in \mathbb{C}^{n \times n}$ is defined as 
\[\rho(\ve{A}) \equiv \max \lbrace |\lambda|, \lambda \in \sigma(\ve{A}) \rbrace\]
With the properties:
%
\begin{itemize}
\item $\rho(\ve{A})^k = \rho(\ve{A}^k)$
\item $\rho(\ve{A}) = \rho(\ve{A}^T)$
\item $\rho(\xi \ve{A}) = |\xi| \rho(\ve{A})$
\end{itemize}
%
Let's think about the meaning of the spectral radius. This is the largest change a matrix can induce on a vector while maintaining it's direction. That is what the matrix 2-norm means, which is analogous to the vector idea of length.

%\textbf{example}
%Show that the eigenvalues of a Hermitian matrix are real, noting that the eigenvalues of $\ve{A}$ satisfy
%\[\ve{A}\vec{x} = \lambda x \qquad \text{and } \vec{x} \neq \vec{0}\]
%and recall that $\ve{A} = \ve{A}^H = \ccm{A}^T$ defines a Hermitian matrix.
%%
%\begin{align}
%\cc{u}^T \ccm{A}^T &= \cc{\lambda} \cc{u}^T
%\qquad \text{The first step} \nonumber \\
%%
%\cc{u}^T \ve{A} &= \cc{\lambda} \cc{u}^T 
%\qquad \text{apply Hermitian definition}\nonumber \\
%%
%\cc{u}^T \ve{A} \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
%\qquad \text{right multiply by }\vec{u}\nonumber \\
%%
%\cc{u}^T \lambda \vec{u} &= \cc{\lambda} \cc{u}^T \vec{u}
%\qquad \text{apply eigenvalue definition }\nonumber \\
%%
%\lambda &= \cc{\lambda} 
%\qquad \text{recognize that the }\vec{u}\text{s factor out}\nonumber
%\end{align}


\end{document}